{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e3cf84",
   "metadata": {},
   "source": [
    "# Chapter 5: Estimation theory\n",
    "\n",
    "```{epigraph}\n",
    "*\"Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.\"*\n",
    "\n",
    "-- Lao Tzu\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "- [Estimation theory](https://en.wikipedia.org/wiki/Probability_theory)\n",
    "```\n",
    "\n",
    "## Sufficient statistics\n",
    "\n",
    "A statistic is a function $T=g\\left(X_{1}, \\dots, X_{n}\\right)$ of the random sample $X_{1}, \\dots, X_{n}$ generated from a probability distribution with density $f(X \\mid \\theta)$. \n",
    "\n",
    "````{prf:example} 5.1\n",
    ":nonumber:\n",
    ":label: chap5_1\n",
    "The followings are statistics, \n",
    "- sample average: $T=\\frac{1}{n} \\sum X_{i}$\n",
    "- sample median: $T= median \\left(X_{1}, \\dots, X_{n}\\right)$\n",
    "- sample maximum: $T=\\max \\left(X_{1}, \\dots, X_{n}\\right)$\n",
    "````\n",
    "\n",
    "We use statistics to estimate unknown parameters. $\\mathrm{T}$ is a sufficient statistic if the statistician who knows the value of $\\mathrm{T}$ can do just as good a job of estimating the unknown parameter $\\theta$ as the statistician who knows the entire random sample. \n",
    "\n",
    "````{prf:definition} sufficient statistics\n",
    ":nonumber:\n",
    ":label: sufficient\n",
    "A statistic $\\mathrm{T}$ is sufficient for parameter $\\theta$ if the conditional distribution of a random sample $X_{1}, \\dots, X_{n}$, given $\\mathrm{T}$, does not depend on $\\theta$, i.e.,\n",
    "\n",
    "$$\n",
    "f\\left(X_{1}, \\dots, X_{n} \\mid T, \\theta\\right)=f\\left(X_{1}, \\dots, X_{n} \\mid T\\right)\n",
    "$$\n",
    "````\n",
    "\n",
    "````{prf:theorem} Factorization theorem\n",
    ":nonumber:\n",
    ":label: 5.1\n",
    "If the probability density function of data $X$ is $f(X \\mid \\theta)$, then $\\mathrm{T}$ is sufficient for $\\theta$ if and only if nonnegative functions $g$ and $h$ can be found such that \n",
    "\n",
    "$$f(X \\mid \\theta)=h(x) g(T, \\theta)$$\n",
    "````\n",
    "\n",
    "If $z(x)$ is a one-to-one function and $\\mathrm{T}$ is a sufficient statistic, then $z(T)$ is a sufficient statistic. For example, if the statistic $\\mathrm{T}$ is sufficient, then $\\mathrm{2T}$ is also a sufficient statistic.\n",
    "\n",
    "````{prf:example} 5.2\n",
    ":nonumber:\n",
    ":label: chap5_2\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Bernoulli}(p)$ , find the sufficient statistic for $p$. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(X_{1},\\dots, X_{n}|p\\right) &=\\prod p^{x_{i}}(1-p)^{1-x_{i}}\\\\\n",
    "&=p^{\\sum x_{i}}(1-p)^{n-\\sum x_{i}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "Thus the sufficient statistic for $p$ is $\\sum x_{i}$.\n",
    "````\n",
    "\n",
    "````{prf:example} 5.3\n",
    ":nonumber:\n",
    ":label: chap5_3\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Normal}\\left(\\mu, \\sigma^{2}\\right)$, find the sufficient statistics for $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "The joint density function of $X_{1}, \\dots, X_{n}$ is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(X_{1}=x_{1}, \\dots, X_{n}=x_{n} \\mid \\mu, \\sigma^{2}\\right) &=\\prod_{i=1}^{n} f\\left(X_{i}=x_{i} \\mid \\mu, \\sigma^{2}\\right)\\\\\n",
    "&=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}} \\\\\n",
    "& =\\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)^{n} e^{-\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}}\\\\\n",
    "&=\\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)^{n} e^{-\\frac{\\sum_{i=1}^{n} x_{i}^{2}-2 \\mu \\sum_{i=1}^{n} x_{i}+n \\mu^{2}}{2 \\sigma^{2}}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, the sufficient statistics for $\\left(\\mu, \\sigma^{2}\\right)$ are $\\left(\\sum_{i=1}^{n} x_{i}, \\sum_{i=1}^{n} x_{i}^{2}\\right)$.\n",
    "````\n",
    "\n",
    "````{prf:example} 5.4\n",
    ":nonumber:\n",
    ":label: chap5_4\n",
    "\n",
    "Given a random sample $X_{1},\\dots, X_{n} \\sim$ Poisson $(\\lambda)$, find the sufficient statistic for $\\lambda$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(X_{1}=x_{1}, \\dots, X_{n}=x_{n} \\mid \\lambda\\right) &= \\prod_{i=1}^{n} f\\left(X_{i}=x_{i} \\mid \\lambda\\right)\\\\\n",
    "&=\\prod_{i=1}^{n} \\frac{1}{x_{i}!}\\lambda^{x_{i}} e^{-\\lambda} \\\\\n",
    "&=\\lambda^{\\sum_{i=1}^{n} x_{i}} e^{-\\lambda n} \\prod_{i=1}^{n} \\frac{1}{x_{i}!}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, the sufficient statistic for $\\lambda$ is $\\sum_{i=1}^{n} x_{i}$.\n",
    "````\n",
    "\n",
    "````{prf:example} 5.5\n",
    ":nonumber:\n",
    ":label: chap5_5\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim$ Exponential $(\\lambda)$, find the sufficient statistic for $\\lambda$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(X_{1}=x_{1}, \\dots, X_{n}=x_{n} \\mid \\lambda\\right) &=\\prod_{i=1}^{n} f\\left(X_{i}=x_{i} \\mid \\lambda\\right)\\\\\n",
    "&=\\prod_{i=1}^{n} \\lambda e^{-\\lambda x_{i}}\\\\\n",
    "&=(\\lambda)^{n} e^{-\\lambda \\sum_{i=1}^{n} x_{i}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "Thus, the sufficient statistic for $\\lambda$ is $\\sum_{i=1}^{n} x_{i}$.\n",
    "````\n",
    "\n",
    "````{prf:example} 5.6\n",
    ":nonumber:\n",
    ":label: chap5_6\n",
    "\n",
    "Let $X_{(1)}, \\dots, X_{(n)}$ be the order statistics of a random sample $X_{1}, \\dots, X_{n} \\sim f(x \\mid \\theta)$. Given the order statistics, the distribution of data $X_1, \\dots, X_n$, i.e., \n",
    "\n",
    "$$f\\left(X_{1}, \\dots, X_{n} \\mid X_{(1)}, \\ldots, X_{(n)},\\theta\\right)$$\n",
    "\n",
    "is a discrete uniform distribution, which does not depend on parameters $\\theta$. Thus, the order statistics $X_{(1)}, \\dots, X_{(n)}$ are sufficient statistics for parameters $\\theta$.\n",
    "````\n",
    "\n",
    "## Unbiased estimator\n",
    "````{prf:definition} unbiased estimator\n",
    ":nonumber:\n",
    ":label: unbiased\n",
    "An estimator $\\hat{\\theta}$ is unbiased if and only if $E(\\hat{\\theta})=\\theta$.\n",
    "````\n",
    "\n",
    "````{prf:example} 5.7\n",
    ":nonumber:\n",
    ":label: chap5_7\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Normal}\\left(\\mu, \\sigma^{2}\\right)$, the sample average $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$ is an unbiased estimator of $\\mu$.\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "E(\\bar{X}) &= E(\\frac{1}{n}\\sum_{i=1}^nX_i)\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^nE(X_i)\\\\\n",
    "&= \\frac{1}{n}n\\mu = \\mu\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "````\n",
    "\n",
    "$\\left(X_{1}+X_{2}\\right) / 2$ is another unbiased estimator of $\\mu$.\n",
    "\n",
    "````{prf:example} 5.8\n",
    ":nonumber:\n",
    ":label: chap5_8\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim$ Exponential $(\\lambda)$, the sample average is an unbiased estimator of $\\lambda$.\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "E(\\bar{X}) &= E(\\frac{1}{n}\\sum_{i=1}^nX_i)\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^nE(X_i)\\\\\n",
    "&= \\frac{1}{n}n\\lambda = \\lambda\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "````\n",
    "\n",
    "````{prf:theorem}\n",
    ":nonumber:\n",
    ":label: 5.2\n",
    "Given a random sample $X_{1}, \\dots, X_{n}$, the sample average $\\bar{X}$ is an unbiased estimator of the population mean $\\theta$.\n",
    "````\n",
    "\n",
    "## Mean squared errors\n",
    "\n",
    "We introduce the mean squared error (MSE) to evaluate the performance of an estimator. A good estimator should have a small MSE.\n",
    "\n",
    "$$MSE = E\\left[(\\hat{\\theta}-\\theta)^{2}\\right]$$\n",
    "\n",
    "````{prf:theorem}\n",
    ":nonumber:\n",
    ":label: 5.3\n",
    "\n",
    "The MSE is the sum of the bias and variance. \n",
    "\n",
    "$$\n",
    "E\\left[(\\hat{\\theta}-\\theta)^{2}\\right]=(E(\\hat{\\theta})-\\theta)^{2}+\\operatorname{var}(\\hat{\\theta})\n",
    "$$\n",
    "````\n",
    "\n",
    "If we only consider unbiased estimators, i.e., $(E(\\hat{\\theta})-\\theta)^{2}=0$, then we choose the estimator with the minimum variance.\n",
    "\n",
    "## Method of moments\n",
    "By the law of large numbers, the sample average is a good estimator of the population average. Thus, we can use \n",
    "\n",
    "- $\\hat{E(X)}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}$ \n",
    "- $\\hat{E\\left(X^{2}\\right)}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}$\n",
    "- $\\hat{E\\left(X^{k}\\right)}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}{ }^{k}$\n",
    "\n",
    "\n",
    "````{prf:example} 5.9\n",
    ":nonumber:\n",
    ":label: chap5_9\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Normal}\\left(\\mu, \\sigma^{2}\\right)$, find the moment estimators of the parameters $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "Because $E(X)=\\mu$, the parameter $\\mu$ is the population mean. Thus, the moment estimate of $\\mu$ is the sample average \n",
    "\n",
    "$$\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$$ \n",
    "\n",
    "In addition, $\\operatorname{var}(X)=E\\left(X^{2}\\right)-E(X)^{2}$. The moment estimate of the population mean $E(X)$ is the sample average $\\bar{X}$. Similarly, the moment estimate of the population average $E\\left(X^{2}\\right)$ is the sample average of $x^2$, i.e., $\\hat{E\\left(X^{2}\\right)} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}^{2}$. Thus, the moment estimate of the variance $\\sigma^2$ is \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}-\\bar{x}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\n",
    "$$\n",
    "````\n",
    "\n",
    "\n",
    "## Maximum likelihood estimator\n",
    "\n",
    "Let $X_{1}, \\dots, X_{n}$ is a random sample generated from a discrete probability distribution with the probability mass function $P(x \\mid \\theta)$. We want to estimate parameter $\\theta$. \n",
    "\n",
    "````{prf:definition} likelihood\n",
    ":nonumber:\n",
    ":label: likelihood\n",
    ":nonumber:\n",
    "\n",
    "The joint probability mass function \n",
    "\n",
    "$$P\\left(X_{1}, \\dots, X_{n} \\mid \\theta\\right)=\\prod_{i=1}^{n} P\\left(X_{i} \\mid \\theta\\right)$$ \n",
    "\n",
    "is also called the likelihood function. The likelihood function represents the probability (or likelihood) of the observed data $X_{1}, \\dots, X_{n}$, given a certain value of $\\theta$.\n",
    "````\n",
    "\n",
    "Suppose that $\\theta$ can take on three values $1, 2 ,3$.\n",
    "\n",
    "$$P\\left(X_{1}, \\dots, X_{n} \\mid \\theta=1\\right)=0.3$$\n",
    "\n",
    "$$P\\left(X_{1}, \\dots, X_{n} \\mid \\theta=2\\right)=0.4$$\n",
    "\n",
    "$$P\\left(X_{1}, \\dots, X_{n} \\mid \\theta=3\\right)=0.1$$\n",
    "\n",
    "```{admonition} Intuition\n",
    "We estimate $\\theta$ by the value that can maximize the likelihood of the observed data $X_{1}, \\ldots, X_{n}$. This is called the maximum likelihood estimator of $\\theta$. Thus, $\\hat{\\theta}=2$.\n",
    "```\n",
    "\n",
    "For continuous random variables, the likelihood function is the joint density function of data.The maximum likelihood estimator is the value of parameter that can maximize the likelihood function.\n",
    "\n",
    "\n",
    "````{prf:example} 5.10\n",
    ":nonumber:\n",
    ":label: chap5_10\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Normal}\\left(\\mu, \\sigma^{2}\\right)$, find the MLE of the population mean $\\mu$, assuming the variance $\\sigma^2$ is given. \n",
    "\n",
    "We first find the likelihood function which is just the joint density function of $X_{1}, \\dots, X_{n}$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\mu) = f\\left(X_{1}=x_{1},\\dots, X_{n}=x_{n} \\mid \\mu, \\sigma^{2}\\right) &=\\prod_{i=1}^{n} f\\left(X_{i}=x_{i} \\mid \\mu, \\sigma^{2}\\right) \\\\\n",
    "&=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}} \\\\\n",
    "&=\\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)^{n} e^{-\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Next, we find the loglikelihood because the loglikelihood has the same maximizor and it is easier to find the maximizor of the loglikelihood function.\n",
    "\n",
    "$$\n",
    "log(L(\\mu)) = -\\frac{n}{2}log\\left(2 \\pi \\sigma^{2} \\right) - \\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}\n",
    "$$\n",
    "\n",
    "To maximize the loglikelihood function, we take the first derivate and set it to be 0\n",
    "\n",
    "$$\n",
    "\\frac{\\partial log(L(\\mu)) }{\\mu} = \\frac{2\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)}{2 \\sigma^{2}} = 0 \n",
    "$$\n",
    "\n",
    "Solving the equation, we find $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$. Thus, the maximum likelihood estimate of $\\mu$ is the sample average, i.e., $\\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n",
    "\n",
    "````\n",
    "\n",
    "````{prf:example} 5.11\n",
    ":nonumber:\n",
    ":label: chap5_11\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Poisson}(\\lambda)$, find the maximum likelihood estimate of the parameter $\\lambda$.\n",
    "\n",
    "We first find the likelihood function which is just the joint density function of $X_{1}, \\dots, X_{n}$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(X_{1}=x_{1},\\dots, X_{n}=x_{n} \\mid \\lambda\\right) &=\\prod_{i=1}^{n} f\\left(X_{i}=x_{i} \\mid \\lambda\\right) \\\\\n",
    "&=\\prod_{i=1}^{n} \\lambda^{x_{i}} e^{-\\lambda} \\frac{1}{x_{i}!} \\\\\n",
    "&=\\lambda^{\\sum_{i=1}^{n} x_{i}} e^{-\\lambda n} \\prod_{i=1}^{n}\\frac{1}{x_{i}!} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Next, we find the loglikelihood because the loglikelihood has the same maximizor and it is easier to find the maximizor of the loglikelihood function.\n",
    "\n",
    "$$\n",
    "log(L(\\lambda)) = \\sum_{i=1}^{n}x_{i}log(\\lambda) - n\\lambda - log\\left(\\prod_{i=1}^{n}\\frac{1}{x_{i}!}  \\right)\n",
    "$$\n",
    "\n",
    "To maximize the loglikelihood function, we take the first derivate and set it to be 0\n",
    "\n",
    "$$\n",
    "\\frac{\\partial log(L(\\lambda)) }{\\lambda} = \\frac{1}{\\lambda}\\sum_{i=1}^{n}x_{i}-n = 0 \n",
    "$$\n",
    "\n",
    "Solving the equation, we find $\\lambda = \\frac{1}{n}\\sum_{i=1}^n x_i$. Thus, the maximum likelihood estimate of $\\lambda$ is the sample average, i.e., $\\hat{\\lambda}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n",
    "\n",
    "````\n",
    "\n",
    "````{prf:example} 5.12\n",
    ":nonumber:\n",
    ":label: chap5_12\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim$ Exponential $(\\lambda)$, find the maximum likelihood estimate of the parameter $\\lambda$.\n",
    "\n",
    "We first find the likelihood function which is just the joint density function of $X_{1}, \\dots, X_{n}$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(X_{1}=x_{1},\\dots, X_{n}=x_{n} \\mid \\lambda\\right) &=\\prod_{i=1}^{n} f\\left(X_{i}=x_{i} \\mid \\lambda\\right)\\\\\n",
    "&=\\prod_{i=1}^{n} \\frac{1}{\\lambda} e^{-\\frac{x_{i}}{\\lambda}} \\\\\n",
    "& =\\lambda^{-n} e^{-\\frac{\\sum_{i=1}^{n} x_{i}}{\\lambda}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Next, we find the loglikelihood because the loglikelihood has the same maximizor and it is easier to find the maximizor of the loglikelihood function.\n",
    "\n",
    "$$\n",
    "log(L(\\lambda)) = -nlog(\\lambda) - \\frac{\\sum_{i=1}^{n}x_{i}}{\\lambda}\n",
    "$$\n",
    "\n",
    "To maximize the loglikelihood function, we take the first derivate and set it to be 0\n",
    "\n",
    "$$\n",
    "\\frac{\\partial log(L(\\lambda)) }{\\lambda} = -\\frac{n}{\\lambda}+\\frac{\\sum_{i=1}^{n}x_{i}}{\\lambda^2} = 0 \n",
    "$$\n",
    "\n",
    "Solving the equation, we find $\\lambda = \\frac{1}{n}\\sum_{i=1}^n x_i$. Thus, the maximum likelihood estimate of $\\lambda$ is the sample average, i.e., $\\hat{\\lambda}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n",
    "````\n",
    "\n",
    "\n",
    "## Confidence intervals\n",
    "````{prf:definition} confidence interval\n",
    ":nonumber:\n",
    ":label: confidence_interval\n",
    ":nonumber:\n",
    "An interval $[a,b]$ is said to be the $\\alpha \\%$ ($0\\le \\alpha \\le 100$) confidence interval for the parameter $\\theta$, if $P(a<\\theta<b) = \\alpha \\%$.\n",
    "````\n",
    "We hope to find an interval with a high confidence level. The confidence level increases as the interval gets wider. However, the interval becomes useless if it is too wide, even thought the confidence level is very high. For example, $[-\\infty,\\infty]$ is a 100% confidence interval because we are 100% sure that the parameter value is between $-\\infty$ and $\\infty$, but this interval does not provide any useful information about the parameter.\n",
    "\n",
    "```{important}\n",
    "We would like to construct the 95% confidence interval.\n",
    "``` \n",
    "\n",
    "````{prf:example} 5.13\n",
    ":nonumber:\n",
    ":label: chap5_13\n",
    "\n",
    "Given a random sample $X_{1}, \\dots, X_{n} \\sim \\operatorname{Normal}\\left(\\mu, \\sigma^{2}\\right)$, find the 95% confidence interval $[a, b]$ such that $P(a \\leq \\mu \\leq b)=0.95$. \n",
    "\n",
    "\n",
    "The sample average $\\bar{x}$ has the normal distribution with mean $\\mu$ and variance $\\sigma^{2} / n$. Thus, $\\frac{\\sqrt{n}(\\bar{x}-\\mu)}{\\sigma}$ has a standard normal distribution. \n",
    "\n",
    "$$P\\left(-2 \\leq \\frac{\\sqrt{n}(\\bar{x}-\\mu)}{\\sigma} \\leq 2\\right)=0.95$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "P\\left(\\bar{x}-\\frac{2 \\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x}+\\frac{2 \\sigma}{\\sqrt{n}}\\right)=0.95\n",
    "$$\n",
    "\n",
    "Thus, the 95% confidence interval for $\\mu$ is $\\left[\\bar{x}-\\frac{2 \\sigma}{\\sqrt{n}}, \\bar{x}+\\frac{2 \\sigma}{\\sqrt{n}}\\right]$, in which the population standard deviation $\\sigma$ can be replaced by its maximum likelihood estimate - the sample standard deviation $\\hat{\\sigma} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}$.\n",
    "````\n",
    "\n",
    "The above example can be generalized to the construction of 95% CI for any parameter using its maximum likelihood estimate, as demonstrated by the following theorem.\n",
    "\n",
    "````{prf:theorem}\n",
    ":nonumber:\n",
    ":label: 5.4\n",
    "\n",
    "If the sample size is large, the 95% CI for a parameter $\\theta$ is\n",
    "\n",
    "$$\n",
    "\\left[\\hat{\\theta}_{MLE}-2sd\\left(\\hat{\\theta}_{MLE}\\right), \\hat{\\theta}_{MLE}+2sd\\left(\\hat{\\theta}_{MLE}\\right)\\right]\n",
    "$$\n",
    "````\n",
    "\n",
    "Note that confidence intervals are random variables. Different samples generate different confidence intervals. Thus, the $95 \\%$ confidence interval is interpreted as on average $95 \\%$ of confidence intervals covers true $\\mu$. It is not correct to say that the probability that $\\mu$ is in the interval is $0.95$.\n",
    "\n",
    "## Convergence Theorems\n",
    "\n",
    "````{prf:definition} convergence almost surely\n",
    ":label: def_5.5\n",
    ":nonumber:\n",
    "To say that a sequence $\\left\\{X_{n}\\right\\}$ of random variables converges almost surely towards $X$ if \n",
    "\n",
    "$$P\\left(\\lim_{n \\rightarrow \\infty} X_{n}=X\\right)=1$$\n",
    "````\n",
    "\n",
    "````{prf:definition} convergence in probability\n",
    ":label: def_5.6\n",
    ":nonumber:\n",
    "A sequence $\\left\\{X_{n}\\right\\}$ of random variables converges in probability towards the random variable $X$ if for all $\\varepsilon>0$, \n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} P\\left(\\left|X_{n}-X\\right| \\leq \\varepsilon\\right)=1$$\n",
    "````\n",
    "\n",
    "````{prf:definition} convergence in distribution\n",
    ":label: 5.7\n",
    ":nonumber:\n",
    "A sequence $\\left\\{X_{n}\\right\\}$ of random variables is said to converge in distribution to a random variable $X$ if \n",
    "\n",
    "$$\\lim _{n \\rightarrow \\infty} F_{n}(x)=F(x)$$\n",
    "\n",
    "where $F_{n}(x)$ is the probability distribution function of $X_{n}$.\n",
    "````\n",
    "\n",
    "Convergence a.s $\\Rightarrow$ Convergence in probability $\\Rightarrow$ convergence in distribution. But the reverse is not correct. \n",
    "\n",
    "````{prf:proposition} Markov inequality\n",
    "If $X$ is a non-negative random variable and $a>0$, then $P(X \\geq a) \\leq \\frac{E(X)}{a}$. \n",
    "````\n",
    "````{prf:proposition} Chebyshev's inequality\n",
    "$P(|X-u| \\geq d) \\leq \\frac{\\sigma^{2}}{d^2}$\n",
    "````\n",
    "\n",
    "These inequalities are satisfied for all probability distributions. When the underlying probability distribution is given, we can exactly calculate $P(|X-u| \\geq d)$; then there is no need to find the upper limit given by the Chebyshev's inequality.\n",
    "\n",
    "````{prf:theorem} The weak Law of large numbers\n",
    ":nonumber:\n",
    ":label: the_5.5\n",
    "The sample average converges in probability to the population mean as the sample size $n$ goes to infinity, regardless of the underlying probability distribution.\n",
    "\n",
    "\n",
    "$$\n",
    "\\bar{X} \\stackrel{\\text { in prob }}{\\longrightarrow} \\mu \\text { as } n \\rightarrow \\infty\n",
    "$$\n",
    "````\n",
    "\n",
    "````{prf:theorem} Central Limit Theorem\n",
    ":nonumber:\n",
    ":label: the_5.6\n",
    "\n",
    "The sample average converges in distribution to a normal random variable $X$, regardless of the underlying probability distribution\n",
    "\n",
    "$$\n",
    "\\bar{X} \\stackrel{\\text { in distribution }}{\\longrightarrow} X \\text { as } n \\rightarrow \\infty\n",
    "$$\n",
    "````\n",
    "\n",
    "## Simulation\n",
    "\n",
    "Simulation is a procedure of drawing samples from the population using computers. If the population (or the probability distribution) is given, generating random numbers from the given probability distribution is equivalent to repeating real experiments in the lab to collect multiple samples.\n",
    "\n",
    "```{tip}\n",
    "Due to the law of large numbers, simulation-based numerical approaches are able to approximate expectations and probabilities using sample averages or proportions.\n",
    "```\n",
    "\n",
    "Example: Suppose $X$ is a normal $\\left(\\mu=1, \\sigma^{2}=1\\right)$ random variable. To calculate $E(\\log (X))$, we can simulate 1000 random numbers from normal $\\left(\\mu=1, \\sigma^{2}=1\\right)$ and use the sample average of $\\left(\\log \\left(X_{1}\\right), \\ldots, \\log \\left(X_{1000}\\right)\\right)$ to approximate $E(\\log (X))$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}