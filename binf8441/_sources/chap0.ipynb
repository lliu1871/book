{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4a33a6",
   "metadata": {},
   "source": [
    "# Chapter 0: Prerequisites\n",
    "\n",
    "```{epigraph}\n",
    "*\"Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.\"*\n",
    "\n",
    "-- Lao Tzu\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "- [Calculus](https://www.britannica.com/science/real-number)\n",
    "```\n",
    "\n",
    "## Calculus\n",
    "\n",
    "### Derivative\n",
    "````{prf:definition} derivative\n",
    ":nonumber:\n",
    ":label: derivative\n",
    "The derivative $f^{\\prime}(x)$ of a differentiable function $f(x)$ at $x$ is defined as the limit\n",
    " \n",
    "$$f^{\\prime}(x)=\\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}$$\n",
    "````\n",
    "The derivative $f^{\\prime}(a)$ is the instantaneous rate of change of $y=f(x)$ at $x=a$. If $f(x)$ is the position of an object at  time $x$ then the derivate $f^{\\prime}(a)$ is the velocity of the object at $x=a$. \n",
    "\n",
    "Geometrically, the derivate of $f(x)$ at $x=a$ is the slope of the tangent line of the graph $y=f(x)$ at $x=a$. The equation of the tangent line is given by \n",
    "\n",
    "$$y=f(a)+f^{\\prime}(a)(x-a)$$\n",
    "\n",
    "A function $f(x)$ is said to be differentiable at $x=a$ if the derivative $f^{\\prime}(a)$ exists. If $f(x)$ is differentiable at any point in an open set $D\\subset\\mathbb{R}$, we say that $f(x)$ is differentiable on $D$.  \n",
    "\n",
    "```{admonition} Notation\n",
    "If $y=f(x)$, then all of the following are equivalent notations for the derivative. $f^{\\prime}(x)=y^{\\prime}=\\frac{d f}{d x}=\\frac{d y}{d x}=\\frac{d}{d x}(f(x))=D_xf$ \n",
    "\n",
    "If $y=f(x)$, then all of the following are equivalent notations for derivative evaluated at $x=a$. $f^{\\prime}(a)=\\left.y^{\\prime}\\right|_{x=a}=\\left.\\frac{d f}{d x}\\right|_{x=a}=\\left.\\frac{d y}{d x}\\right|_{x=a}=D_af$\n",
    "```\n",
    "\n",
    "Note that if $f$ is a constant function, then its derivative is equal to 0. \n",
    "\n",
    "````{prf:theorem} linearity of derivative\n",
    "Given two functions $f$ and $g$ that are differentiable at a point $a$, the derivate of the sum of two functions is equal to the sum of the derivatives of two functions, i.e., \n",
    "\n",
    "$$D_a(f+g)=D_af+D_ag$$\n",
    "\n",
    "Suppose $c$ is a real number, then \n",
    "\n",
    "$$D_a(cf)=cD_af$$\n",
    "````\n",
    "\n",
    "- **The product rule**: $(f g)^{\\prime}=f^{\\prime} g+f g^{\\prime}$\n",
    "- **The quotient rule**: $\\left(\\frac{f}{g}\\right)^{\\prime}=\\frac{f^{\\prime} g-f g^{\\prime}}{g^{2}}-$\n",
    "- **The power rule**: $\\frac{d}{d x}\\left(x^{n}\\right)=n x^{n-1}$\n",
    "- **The chain rule**: $\\frac{d}{d x}(f(g(x)))=f^{\\prime}(g(x)) g^{\\prime}(x)$\n",
    "\n",
    "```{admonition} Common Derivatives\n",
    "- $(x)^{\\prime}=1$\n",
    "- $\\left(x^n\\right)^{\\prime} = nx^{n-1}$\n",
    "- $\\left(a^{x}\\right)^\\prime=a^{x} \\ln (a)$\n",
    "- $\\left(e^x\\right)^\\prime=e^x$\n",
    "- $(\\ln x)^\\prime=\\frac{1}{x}$\n",
    "- $(\\sin x)^\\prime=\\cos x$\n",
    "- $(\\cos x)^\\prime=-\\sin x$\n",
    "```\n",
    "\n",
    "````\\{prf:example\\} 0.1\n",
    ":nonumber:\n",
    ":label: 0.1\n",
    ":nonumber:\n",
    "\n",
    "Find the derivative $\\frac{de^{2x}}{dx}$\n",
    "\n",
    "Let $y=2x$ and by the chain rule we have\n",
    "$\\frac{de^{2x}}{dx}=\\frac{(de^y)}{dy}\\frac{dy}{dx}=e^{y}*2=2e^{2x}$\n",
    "````\n",
    "\n",
    "Derivatives can be used to minimize or maximize a differentiable function $f(x)$. To find the value of $x$ that can maximize or minimize the function $f(x)$, we set the first derivative to be 0, i.e., $f^{\\prime}(x)=0$ and check the second derivative of $f(x)$.\n",
    "\n",
    "````\\{prf:example\\} 0.2\n",
    ":nonumber:\n",
    ":label: 0.2\n",
    ":nonumber:\n",
    "Find the value of $x$ that can minimize $f(x)=x^2$\n",
    "Set the first derivate to be 0, i.e., $f^{\\prime}(x)=0$, which indicates that $2x=0$ or $x=0$. Since the second derivative $f^{''} (x)=2$ is greater than 0, the value $x=0$ minimizes the function $f(x)=x^2$.\n",
    "````\n",
    "\n",
    "### Integral\n",
    "````{prf:theorem} Fundamental Theorem of Calculus\n",
    ":label: fundamental theorem of calculus\n",
    "\n",
    "If $F(x)$ is an anti-derivative of $f(x)$, then \n",
    "\n",
    "$$\\int_{a}^{b} f(x) d x=F(b)-F(a)$$\n",
    "````\n",
    "\n",
    "```{admonition} Common Integrals \n",
    "\n",
    "- $\\int k d x=k x+c$\n",
    "- $\\int x^{n} d x=\\frac{1}{n+1} x^{n+1}+c$\n",
    "- $\\int \\sin u d u=-\\cos u+c$\n",
    "- $\\int \\cos u d u=\\sin u+c$\n",
    "- $\\int x^{-1} d x=\\int \\frac{1}{x} d x=\\ln |x|+c$\n",
    "- $\\int \\frac{1}{a x+b} d x=\\frac{1}{a} \\ln |a x+b|+c$\n",
    "- $\\int \\ln u d u=u \\ln (u)-u+c$ \n",
    "- $\\int e^u du=e^u+c$\n",
    "- $\\int \\frac{1}{a^{2}+u^{2}} d u=\\frac{1}{a} \\tan ^{-1}\\left(\\frac{u}{a}\\right)+c$\n",
    "- $\\int \\frac{1}{\\sqrt{a^{2}-u^{2}}} d u=\\sin ^{-1}\\left(\\frac{u}{a}\\right)+c$ \n",
    "```\n",
    "### Approximation\n",
    "The continuous functions can be approximated by polynomial functions, which is called Taylor series. The Taylor series to a smooth function $f(x)$ near $x=a$ is \n",
    "\n",
    "$$f(x)=f(a)+f^{'} (a)(x-a)+\\frac{(f^{''} (a) (x-a)^2)}{2}+\\frac{(f^{'''}(a)(x-a)^3)}{3!}+\\dots+\\frac{f^{n}(a)(x-a)^n}{n!}+\\dots$$\n",
    "\n",
    "````\\{prf:example\\} 0.3\n",
    ":nonumber:\n",
    ":label: 0.3\n",
    ":nonumber:\n",
    "Find the Taylor series of $f(x)=e^x$ near $x=0$\n",
    "\n",
    "Because $a=0$, $f(a)=e^a=e^0=1$ and $f^n(a)=e^0=1$. It follows from the above equation that the Taylor series of $e^x$ near $x=0$ is given by \n",
    "\n",
    "$$e^x=1+x+\\frac{x^2}{2}+\\frac{x^3}{3!}+\\dots+\\frac{x^n}{n!}+...$$\n",
    "````\n",
    "## Linear algebra\n",
    "\n",
    "### Dot product\n",
    "\n",
    "````{prf:definition} dot product\n",
    ":nonumber:\n",
    ":label: dot_product\n",
    "The dot product of two vectors $a=(a_1,…,a_n)$ and $b=(b_1,…,b_n)$ is given by \n",
    "\n",
    "$$a\\cdot b=\\sum_{i=1}^na_ib_i$$\n",
    "````\n",
    "\n",
    "````\\{prf:example\\} 0.4\n",
    ":nonumber:\n",
    ":label: 0.4\n",
    ":nonumber:\n",
    "The dot product of two vectors $(1, 2, 3)$ and $(0, 1, 2)$ is $1*0+2*1+3*2=8$. Note that two vectors must have the same length.\n",
    "````\n",
    "\n",
    "````{prf:definition} orthogonal\n",
    ":nonumber:\n",
    ":label: orthogonal\n",
    "Two vectors $a=(a_1,…,a_n)$ and $b=(b_1,…,b_n)$ are orthogonal (perpendicular) if $a\\cdot b=0$. \n",
    "````\n",
    "\n",
    "````\\{prf:example\\} 0.5\n",
    ":nonumber:\n",
    ":label: 0.5\n",
    ":nonumber:\n",
    "Two vectors $(0, 1)$ and $(1, 0)$ are orthogonal because their dot product is equal to 0, i.e., $(0, 1)\\cdot (1, 0) = 0$\n",
    "````\n",
    "\n",
    "Matrices multiplication: $A_{n×k} B_{k×m}=C_{n×m}$, in which $c_{ij}=∑_{w=1}^ka_{iw} b_{wj} $.\n",
    "\n",
    "$$\\begin{pmatrix} 1 & 2\\\\ 3 & 4\\\\ 5 & 6 \\end{pmatrix} \\begin{pmatrix} 1&1\\\\2&2\\end{pmatrix} = \\begin{pmatrix} 5&5\\\\11&11\\\\17&17\\end{pmatrix}$$\n",
    "\t\n",
    "### Linear independent\n",
    "````{prf:definition} linear independent\n",
    ":nonumber:\n",
    ":label: linear_independent\n",
    "Let $\\left(a_1,\\dots,a_n\\right)$ denote the $n$ column vectors of matrix $A$. Those vectors are linearly dependent if and only if there is a non-zero vector $(x_1,\\dots,x_n)$ such that \n",
    "\n",
    "$$x_1 a_1+\\dots+x_n a_n=0$$ \n",
    "````\n",
    "\n",
    "In the matrix $A=\\begin{pmatrix} 1&2\\\\1&2\\end{pmatrix}$, the column vectors $(1, 1)$ and $(2, 2)$ are linearly dependent because $2*(1, 1)-(2, 2) = 0$.\n",
    "\n",
    "The column vectors are linearly independent if and only if the zero vector is the only solution to $x_1 a_1+\\dots+x_n a_n=0$.\n",
    "For example, in the matrix $A=\\begin{pmatrix} 1&0\\\\0&1\\end{pmatrix}$, the column vectors $(1, 0)$ and $(0, 1)$ are linearly independent, because if $x_1 (1, 0)+x_2 (0, 1)=(0, 0)$, then $x_1=0$ and $x_2=0$.\n",
    "\n",
    "### Basis\n",
    "````{prf:definition} basis\n",
    ":nonumber:\n",
    ":label: basis\n",
    "A set of vectors in a vector space $V$ is called a basis, or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set. \n",
    "````\n",
    "\n",
    "For example, the basis of a two-dimensional space could be $(0, 1)$ and $(1, 0)$. It is easy to check that $(0, 1)$ and $(1, 0)$ are linearly independent and every vector $(x, y)$ is a linear combination of two basis vectors, i.e., $(x, y)=x(1, 0)+y(0, 1)$. \n",
    "\n",
    "The basis vectors are not unique, for example, $(-1, 1)$ and $(1, 1)$ are basis vectors because they are linearly independent and every vector in the two-dimensional space is a linear combination of $(-1, 1)$ and $(1, 1)$. In fact, any two linearly independent vectors are the basis of the two-dimensional space.\n",
    "\n",
    "- It can be shown that the $n$ dimensional space has $n$ basis vectors.\n",
    "- The rank of a matrix = the number of linearly independent column vectors\n",
    "- The inverse of a matrix $A$. If we can find a matrix $B$ such that $AB=I$ (identity matrix), matrix $B$ is the inverse of matrix $A$. The inverse of matrix $A$ is often represented by $A^{-1}$. \n",
    "- Determinant of a square matrix $A$, denoted by $\\det(A)$ or $|A|$. The matrix $A$ has a unique inverse if and only if its determinant is not $0$.\n",
    "\n",
    "### Eigenvector\n",
    "````{prf:definition} eigenvector\n",
    ":nonumber:\n",
    ":label: eigenvector\n",
    "Let $A$ be a square matrix. If we can find a non-zero vector $x$ and a number $\\lambda$ such that $Ax=\\lambda x$, the vector $x$ is an eigenvector and the number $\\lambda$ is an eigenvalue.\n",
    "````\n",
    "\n",
    "We need to find $x$ and $\\lambda$ such that $(A-\\lambda I)x=0$. Since $x$ is a non-zero vector, it indicates that $A-λI$ is not invertible, i.e., $\\det⁡(A-\\lambda I)=0$. \n",
    "\n",
    "````{prf:definition} diagonalizable\n",
    ":nonumber:\n",
    ":label: diagonalizable\n",
    "A square matrix $A$ be a square matrix is diagonalizable if it can be written as $A=PDP^{-1}$, in which the column vectors of $P$ are eigenvectors of $A$ and $D$ is a diagonal matrix of eigenvalues.\n",
    "````\n",
    "\n",
    "If a square matrix $A$ is diagonalizable, then it is straightforward to calculate $A^2$,\n",
    "\n",
    "$$A^2 = PDP^{-1}PDP^{-1}=PD^2P^{-1}$$\n",
    "\n",
    "## Statistics\n",
    "The ultimate goal of statistical inference is to understand the unknown **population** through the **samples** generated from the population. The population is characterized by so called **parameters**. We often calculate **statistics** from samples to estimate parameters. \n",
    "\n",
    "```{image} ./images/population.png\n",
    ":alt: population\n",
    ":width: 600px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "````\\{prf:example\\} 0.6\n",
    ":nonumber:\n",
    ":label: 0.6\n",
    ":nonumber:\n",
    "We want to know the average weight of UGA students. We take a random sample of 100 students and calculate their average weight, which is 135 pounds. We conclude that the average weight of UGA students is 135 pounds.\n",
    "\n",
    "- Population: all UGA students\n",
    "- Parameter: the mean weight; population mean\n",
    "- Sample: 100 students\n",
    "- Statistic: the sample average\n",
    "````\n",
    "\n",
    "We use the sample average to estimate the population mean. Does it make sense? What is the probability that the population mean is exactly equal to 135 ? How do we handle uncertainty?\n",
    "\n",
    "Converting a real problem to a mathematical problem\n",
    "1. How do we denote a population in math?\n",
    "population = probability distribution $f(x \\mid \\theta)$\n",
    "\n",
    "2. How do we denote parameter in math?\n",
    "The parameter is a constant $\\theta=g($ population), which is a function of the population\n",
    "\n",
    "3. How do we denote a sample in math?\n",
    "They are random numbers generated from the probability distribution, i.e., $\\boldsymbol{x}_{\\mathbf{1}}, \\ldots, \\boldsymbol{x}_{\\boldsymbol{n}} \\sim \\boldsymbol{f}(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})$\n",
    "\n",
    "4. How do we denote a statistic in math?\n",
    "A statistic is any function of the sample\n",
    "\n",
    "Important conclusions\n",
    "1. If we know the population, i.e., the probability distribution, we do not need to generate real data in the wet lab. We can use computer to simulate data from the probability distribution.\n",
    "2. If we have data, we can make inference about the population (i.e., the probability distribution) from data.\n",
    "\n",
    "```{image} ./images/inference.png\n",
    ":alt: inference\n",
    ":width: 600px\n",
    ":align: center\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}