{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d4cefa",
   "metadata": {},
   "source": [
    "# Chapter 12: Bayesian analysis\n",
    "\n",
    "```{epigraph}\n",
    "*\"Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.\"*\n",
    "\n",
    "-- Lao Tzu\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "- [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference)\n",
    "```\n",
    "\n",
    "In the Bayesian framework, probability is\n",
    "opinion, and that inference from data is nothing other than the revision\n",
    "of such opinion in the light of relevant new information.\n",
    "\n",
    "````{prf:example}\n",
    ":nonumber:\n",
    "There are two boxes in the room: 5 red balls and 5 blue balls\n",
    "in box1; 1 red ball and 9 blue balls in box2. Someone randomly selected\n",
    "a box and took 4 balls with replacement from the box. Suppose all\n",
    "selected balls are blue. We want to know the box from which the balls\n",
    "were selected.\n",
    "\n",
    "![](./images/boxes.png)\n",
    "\n",
    "Let $X$ be the number of blue balls selected from the box. Given box1, X\n",
    "follows Binomial(n=4, p=0.5). Given box2, $X$ follows Binomial(n=4,\n",
    "p=0.9). We assume that two boxes are equally likely to be selected,\n",
    "i.e., $P(box1) = P(box2) = 0.5$. \n",
    "\n",
    "Given the observed data $X=4$, we want to calculate the probability $P(box1 | X=4)$ \n",
    "and the probability $P(box2| X=4)$. By the Bayes rule,\n",
    "\n",
    "$$P(box1|X = 4) = \\frac{P(X = 4|box1)P(box1)}{P(X = 4)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(box2|X = 4) = \\frac{P(X = 4|box2)P(box2)}{P(X = 4)}$$\n",
    "\n",
    "We know that\n",
    "\n",
    "$$P(X = 4) = P\\left( X = 4 \\middle| box1 \\right)P(box1) + P\\left( X = 4 \\middle| box2 \\right)P(box2) = 0.3593$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$P(box1|X = 4) = \\frac{0.03125}{0.3593} = 0.087$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(box2|X = 4) = \\frac{0.32805}{0.3593} = 0.913$$\n",
    "\n",
    "We conclude that the 4 blue balls are more likely to be selected from\n",
    "box2. \n",
    "````\n",
    "\n",
    "In this example, the two boxes are the parameter $\\theta = 1\\ or\\ 2$. The probability distribution of data, $P(X|\\theta)$, is\n",
    "called the likelihood function. The probability distribution $P(\\theta = 1) = P(\\theta = 2) = 0.5$ is the prior distribution of the parameter $\\theta$. The Bayesian inference of\n",
    "$\\theta$ is based on the posterior distribution of parameter $\\theta$\n",
    "given data X, i.e.,\n",
    "\n",
    "$$P\\left( \\theta \\middle| X \\right) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}$$\n",
    "\n",
    "For the continuous data $X$, the posterior density of $\\theta$ is given by\n",
    "\n",
    "$$f\\left( \\theta \\middle| X \\right) = \\frac{f(X|\\theta)f(\\theta)}{f(X)}$$\n",
    "\n",
    "The normalizing constant\n",
    "$f(X) = \\int_{- \\infty}^{\\infty}{f\\left( X \\middle| \\theta \\right)f(\\theta)d\\theta}$\n",
    " is often intractable. Numerical approaches\n",
    "(MCMC algorithms) are used to approximate the posterior distribution\n",
    "$f\\left( \\theta \\middle| X \\right)$ of $\\theta$.\n",
    "\n",
    "## Prior distribution \n",
    "\n",
    "### 1. conjugate prior\n",
    "A conjugate prior is an algebraic convenience, giving a close-form expression for the posterior;\n",
    "otherwise numerical approaches may be necessary.\n",
    "\n",
    "| Likelihood |  Conjugate prior  | Posterior|\n",
    "| ---|---|---|\n",
    "|  Normal|                 Normal  |                 Normal|\n",
    " | Uniform |                Pareto   |                Pareto|\n",
    "  |Weibull  |               Inverse gamma  |          Inverse gamma|\n",
    " | Log-normal  |            Normal   |                Normal|\n",
    " | Exponential   |          Gamma   |                 Gamma|\n",
    " | Inverse gamma  |         Gamma  |                  Gamma|\n",
    "|  Gamma        |           Gamma  |                  Gamma\\\n",
    "|  Binomial      |          Beta   |                  Beta|\n",
    "|  Negative binomial |      Beta    |                 Beta|\n",
    "|  Poisson     |            Gamma   |                 Gamma|\n",
    "|  Multinomial   |          Dirichlet    |            Dirichlet|\n",
    "| | | |\n",
    "  \n",
    "\n",
    "### 2. non-informative prior\n",
    "\n",
    "For example, the uniform prior of parameter $\\theta$ is a non-informative prior, because all possible values of $\\theta$ are equally likely, no preference.\n",
    "\n",
    "### 3. Empirical Bayesian\n",
    "The prior distribution $P(\\theta)$ is estimated from data\n",
    "\n",
    "### 4. Sensitivity analysis\n",
    "The Bayesian inference is based on many prior distributions to see if the inference is significantly affected by different priors. If not, it indicates that the Bayesian inference is robust to the prior distribution.\n",
    "\n",
    "## Bayesian estimation\n",
    "\n",
    "Let $\\widehat{\\theta}$ be a Bayesian estimator of $\\theta$ and let\n",
    "$L(\\theta,\\widehat{\\theta})$ be a loss function, such as the squared loss \n",
    "\n",
    "$$L\\left( \\theta,\\widehat{\\theta} \\right) = \\left( \\theta - \\widehat{\\theta}(x) \\right)^{2}$$\n",
    "\n",
    "````{prf:definition} Bayes risk\n",
    ":label: Bayes risk\n",
    "The Bayes risk of $\\widehat{\\theta}\\ $is defined as\n",
    "\n",
    "$$E\\left( L\\left( \\theta,\\widehat{\\theta} \\right) \\right) = \\int_{\\theta}^{}{\\int_{x}^{}{L\\left( \\theta,\\widehat{\\theta}(x) \\right)f\\left( x \\middle| \\theta \\right)f(\\theta)dxd\\theta}}$$\n",
    "````\n",
    "\n",
    "The expectation is taken over the probability distribution of data $X$ and parameter $\\theta$. \n",
    "\n",
    "````{prf:definition} Bayes estimator\n",
    ":label: Bayes estimator\n",
    "An estimator $\\widehat{\\theta}\\ $is said to be a Bayes estimator if it\n",
    "minimizes the Bayes risk among all estimators.\n",
    "````\n",
    "\n",
    "The estimator which minimizes the posterior expected loss\n",
    "$E\\left( L\\left( \\theta,\\widehat{\\theta} \\right)|X \\right)$ for each $X$\n",
    "also minimizes the Bayes risk and therefore is a Bayes estimator. \n",
    "\n",
    "````{prf:example}\n",
    ":nonumber:\n",
    "If the loss function is squared error, the Bayesian estimate\n",
    "$\\widehat{\\theta}$ of $\\theta$ is the posterior mean $E(\\theta|X)$,\n",
    "because the risk function is given by\n",
    "\n",
    "$$E\\left( L\\left( \\theta,\\widehat{\\theta} \\right)|X \\right) = \\int_{\\theta}^{}{\\left( \\theta - \\widehat{\\theta}(x) \\right)^{2}f\\left( \\theta \\middle| x \\right)d\\theta}$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\\frac{\\partial E\\left( L\\left( \\theta,\\widehat{\\theta} \\right)|X \\right)}{\\partial\\widehat{\\theta}(x)} = \\int_{\\theta}^{}{2\\left( \\theta - \\widehat{\\theta}(x) \\right)f\\left( \\theta \\middle| x \\right)d\\theta} = 2E\\left( \\theta \\middle| x \\right) - 2\\widehat{\\theta}(x) = 0$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$${\\widehat{\\theta}}_{Bayes}(x) = E(\\theta|x)$$\n",
    "````\n",
    "\n",
    "## Markov Chain Monte Carlo algorithm\n",
    "\n",
    "Markov chain Monte Carlo (MCMC) methods are a class of algorithms for\n",
    "sampling from a target probability distribution. It can be shown that\n",
    "the samples generated from the MCMC algorithms converge to the target\n",
    "probability distribution.\n",
    "\n",
    "In Bayesian analysis, the target probability distribution is the\n",
    "posterior distribution. Suppose the likelihood function $f(x|\\theta)$\n",
    "and prior $f(\\theta)$ are given. The posterior density of $\\theta$ is\n",
    "given by\n",
    "\n",
    "$$f\\left( \\theta \\middle| x \\right) = \\frac{f(x|\\theta)f(\\theta)}{f(x)}$$\n",
    "\n",
    "Since the normalizing constant $f(x)$ is often intractable, the\n",
    "posterior distribution is known up to the normalizing constant. Because\n",
    "the MCMC algorithms do not require to know the normalizing constant, they can approximate the posterior probability by sampling from the posterior distribution.\n",
    "\n",
    "The commonly used MCMC algorithms include Gibbs sampling and\n",
    "Metropolis-Hastings algorithm. We here describe the\n",
    "Metropolis-Hastings algorithm\n",
    "\n",
    "````{prf:algorithm} Metropolis-Hastings\n",
    ":nonumber:\n",
    "\n",
    "**Input:** the likelihood and prior\n",
    "**Output** a sample generated from the target distribution\n",
    "\n",
    "1. An arbitrary initial value for $\\theta = \\theta_{0}$\n",
    "\n",
    "2.  We update $\\theta$ as follows. Suppose the current value is\n",
    "    $\\theta_{n}$. A new value of $\\theta$ is proposed in the\n",
    "    neighborhood of $\\theta_{n}$. For example, $\\theta_{new}$ is\n",
    "    generated from the uniform distribution on\n",
    "    $\\lbrack\\theta_{n} - c,\\theta_{n} + c\\rbrack$. The proposal\n",
    "    distribution is the probability distribution from which\n",
    "    $\\theta_{new}$ is proposed, written as $P(\\theta_{new}|\\theta_{n})$.\n",
    "    Here, we use the uniform distribution (it is often called random\n",
    "    walk) as the proposal distribution.\n",
    "\n",
    "3.  The newly proposed $\\theta_{new}$ is either accepted or rejected\n",
    "    according to the probability known as the Hastings ratio,\n",
    "\n",
    "$$\\theta_{n + 1} = \\left{ \\begin{matrix}\n",
    "\\theta_{new}\\ with\\ probabilty = \\ min{\\frac{f\\left( \\theta_{new} \\middle| x \\right)P\\left( \\theta_{n} \\middle| \\theta_{new} \\right)}{f\\left( \\theta_{n} \\middle| x \\right)P\\left( \\theta_{new} \\middle| \\theta_{n} \\right)},\\ 1} \\\\\n",
    "\\theta_{n},\\ otherwise \\\\\n",
    "\\end{matrix} \\right.\\ $$\n",
    "\n",
    "4.  Continue to generate $\\theta$ until the algorithm converges\n",
    "````\n",
    "\n",
    "### 1. Burnin\n",
    "Many approaches have been developed for checking the convergence of the MCMC algorithms.\n",
    "A simple method is to make a log-likelihood plot. The log-likelihood\n",
    "continues to increase and then it will become stable at some point,\n",
    "indicating that the MCMC algorithm has converged. The time period before\n",
    "the chain gets converged is called \"burn-in\". The samples generated\n",
    "during burn-in should be discarded.\n",
    "\n",
    "### 2. Subsampling\n",
    "Note that the samples generated from the MCMC algorithms are not random\n",
    "samples. They are dependent of each other, because the new value\n",
    "$\\theta_{new}$ is proposed from the neighborhood of the old value $\\theta_{n}$. To\n",
    "reduce dependency, we subsample $\\theta$, for example, we sample every 1000 $\\theta$.\n",
    "\n",
    "### 3. Bayesian inference\n",
    "Once we have a sample of $\\theta$ generated from the posterior\n",
    "distribution, the Bayesian inference can be based on the generated\n",
    "sample. For example, the posterior mean, a Bayesian estimator of\n",
    "parameter $\\theta$, is approximated by the sample average of $\\theta$\n",
    "generated from the MCMC algorithm.\n",
    "\n",
    "````{prf:example}\n",
    ":nonumber:\n",
    "\n",
    "$(x_{1},\\ldots,x_{n})$ is a random sample generated from the\n",
    "exponential distribution with mean $1/\\lambda$. The prior of $\\lambda$\n",
    "is the exponential distribution with mean 1/2. The posterior\n",
    "distribution of $\\lambda$ given $(x_{1},\\ldots,x_{n})$ is\n",
    "\n",
    "$$f\\left( \\lambda \\middle| X \\right) = \\frac{f(X|\\lambda)f(\\lambda)}{f(X)} = \\frac{\\lambda^{n}e^{- \\lambda\\sum_{i = 1}^{n}x_{i}}*2e^{- 2\\lambda}}{f(X)} = \\frac{2\\lambda^{n}e^{- \\left( \\sum_{i = 1}^{n}x_{i} + 2 \\right)\\lambda}}{f(X)}$$\n",
    "\n",
    "This is a gamma distribution with $\\alpha = n + 1$ and\n",
    "$\\beta = \\sum_{i = 1}^{n}x_{i} + 2$. The posterior mean is\n",
    "$\\frac{\\alpha}{\\beta} = \\frac{n + 1}{\\sum_{i = 1}^{n}x_{i} + 2}$. Thus,\n",
    "the Bayesian estimate of $\\lambda$ is\n",
    "$\\frac{n + 1}{\\sum_{i = 1}^{n}x_{i} + 2}$.\n",
    "\n",
    "Suppose the data is (1.001, 0.065, 0.014, 1.601, 0.288, 0.095, 0.401,\n",
    "0.227, 0.234, 0.488). Then, the Bayesian estimate of $\\lambda$ is\n",
    "\n",
    "$$\\frac{n + 1}{\\sum_{i = 1}^{n}x_{i} + 2} = \\frac{10 + 1}{4.41 + 2} = 1.716$$\n",
    "````\n",
    "\n",
    "Let's use the MCMC algorithm to approximate the posterior distribution\n",
    "$f\\left( \\lambda \\middle| X \\right)$, then calculate the posterior mean.\n",
    "\n",
    "The R code of the MCMC algorithm should have the following functions (1)\n",
    "Likelihood function, (2) Prior, (3) a function for updating $\\lambda$,\n",
    "and (4) a function for accepting or rejecting the proposed\n",
    "$\\lambda_{new}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0459e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MCMC estimate of lambda: 0.978181536016448\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Bayesian estimate of lambda: 1.00478050725172\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nPwDFTe8X37HfAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "D:\\OneDrive\\Github\\Jupyter\\jupyter_book\\binf8441\\_build\\jupyter_execute\\chap12_1_2.png"
      },
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "# mcmc algorithm: likelihood: exp, prior: exp\n",
    "########################################################\n",
    "loglikelihood <- function (x, lambda) {\n",
    "  n <- length(x)\n",
    "  loglike <- n*log(lambda) - sum(x) * lambda\n",
    "  return (loglike)\n",
    "}\n",
    "logprior <- function(lambda, theta) {\n",
    "  logprior <- log(theta) - lambda*theta\n",
    "  return (logprior)\n",
    "}\n",
    "update_lambda<-function(lambda, window_width){\n",
    "  newlambda <- lambda + (2*runif(1)-1) * window_width\n",
    "  return (newlambda)\n",
    "}\n",
    "\n",
    "#######################################################\n",
    "# algorithm\n",
    "########################################################\n",
    "x = rexp(100)\n",
    "samplesize = length(x)\n",
    "theta <- 10 #prior of lambda\n",
    "lambda_theory = (samplesize+1)/(sum(x)+theta)\n",
    "\n",
    "totalround <- 100000\n",
    "lambda <- 1:totalround\n",
    "loglike <- 1:totalround\n",
    "window_width <- 0.05\n",
    "\n",
    "oldlambda <- 1 #initial value of lambda\n",
    "oldloglike <- loglikelihood(x, oldlambda)\n",
    "oldlogprior <- logprior(oldlambda, theta)\n",
    "\n",
    "for (i in 1:totalround)\n",
    "{\n",
    "  newlambda <- update_lambda(oldlambda, window_width)\n",
    "  if (newlambda<0) newlambda = -newlambda\n",
    "  newloglike <- loglikelihood(x, newlambda)\n",
    "  newlogprior <- logprior(newlambda, theta)\n",
    "\n",
    "  hastings_ratio <- min(exp((newloglike+newlogprior)-(oldloglike+oldlogprior)),1)\n",
    "\n",
    "  if(runif(1) < hastings_ratio){\n",
    "    lambda[i] = newlambda\n",
    "    loglike[i] = newloglike\n",
    "    oldlambda = newlambda\n",
    "    oldlogprior = newlogprior\n",
    "  }else{\n",
    "    lambda[i] = oldlambda\n",
    "    loglike[i] = oldloglike\n",
    "  }\n",
    "}\n",
    "\n",
    "plot(loglike,type=\"l\")\n",
    "burnin = totalround/2\n",
    "\n",
    "print(paste(\"MCMC estimate of lambda:\", mean(lambda[burnin:totalround])))\n",
    "print(paste(\"Bayesian estimate of lambda:\",lambda_theory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde616c",
   "metadata": {},
   "source": [
    "## Bayesian hypothesis testing and model selection\n",
    "\n",
    "We assume that data $X$ have arisen from one of the two hypotheses\n",
    "$H_0$ and $H_1$ according to a probability density $P(X|H_{0})$ or\n",
    "$P(X|H_{1})$. \n",
    "\n",
    "Given a priori probabilities $P(H_{0})$ and\n",
    "$P\\left( H_{1} \\right) = 1 - P(H_{0})$, the posterior probability of\n",
    "hypothesis $H_0$ is given by\n",
    "\n",
    "$$P\\left( H_{0} \\middle| X \\right) = \\frac{P(X|H_{0})P(H_{0})}{P(X)}$$\n",
    "\n",
    "Thus, the posterior odds is equal to\n",
    "\n",
    "$$\\frac{P\\left( H_{0} \\middle| X \\right)}{P(H_{1}|X)} = \\frac{P(X|H_{0})P(H_{0})}{P(X|H_{1})P(H_{1})}$$\n",
    "\n",
    "The Bayes factor is $BF_{01} = \\frac{P(X|H_{0})}{P(X|H_{1})}$.\n",
    "When $H_0$ and $H_1$ are equally probable, the posterior odds is equal to\n",
    "the Bayes factor. In addition,\n",
    "\n",
    "$$P\\left( X \\middle| H_{0} \\right) = \\int_{- \\infty}^{\\infty}{P\\left( X \\middle| \\theta_{0},H_{0} \\right)P\\left( \\theta_{0} \\middle| H_{0} \\right)d\\theta_{0}}$$\n",
    "\n",
    "$P\\left( X \\middle| H_{0} \\right)$ is the marginal likelihood under\n",
    "$H_0$. Similarly, $P\\left( X \\middle| H_{1} \\right)$ is the marginal\n",
    "likelihood under $H_1$. Thus, the Bayes factor is the marginal likelihood\n",
    "ratio statistic.\n",
    "\n",
    "The Bayes factor is the summary of the evidence provided by the data in\n",
    "favor of one scientific theory as opposed to another. Interpretation of\n",
    "the Bayes factor\n",
    "\n",
    "  \n",
    "  |$log_{10}(B_{10})$    |      $B_{10}$     |               Evidence against $H_0$|\n",
    "|---|---|---|\n",
    " | 0 to 1/2      |          1 to 3.2      |           Not worth more than a bare mention|\n",
    " | ½ to 1          |        3.2 to 10       |         substantial|\n",
    " | 1 to 2         |         10 to 100    |            Strong|\n",
    " | >2     |                >100     |               decisive|\n",
    " |   |   | |\n",
    "  \n",
    "\n",
    "The marginal likelihoods in the Bayes factor are often intractable. We\n",
    "use numerical approaches to approximate the Bayes factor. A simple\n",
    "approximation method is the harmonic mean approximation,\n",
    "\n",
    "$$\\frac{1}{P\\left( X \\middle| H_{0} \\right)} = \\int_{- \\infty}^{\\infty}{\\frac{1}{P\\left( X \\middle| \\theta_{0},H_{0} \\right)}P\\left( \\theta_{0}|H_{0} \\right)d\\theta_{0}} = E\\left( \\frac{1}{P\\left( X \\middle| \\theta_{0},H_{0} \\right)} \\right)$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\\widehat{P\\left( X \\middle| H_{0} \\right)} = \\left\\lbrack \\frac{1}{n}\\sum_{i = 1}^{m}\\left( \\frac{1}{P\\left( X \\middle| \\theta_{0}^{i},H_{0} \\right)} \\right) \\right\\rbrack^{- 1}$$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "source_map": [
   14,
   266,
   327
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}