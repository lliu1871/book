{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df56c3ef",
   "metadata": {},
   "source": [
    "# Chapter 9: Linear regression \n",
    "\n",
    "```{epigraph}\n",
    "*\"Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.\"*\n",
    "\n",
    "-- Lao Tzu\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "- [Linear regression](https://en.wikipedia.org/wiki/Linear_regression)\n",
    "```\n",
    "\n",
    "## Simple linear regression\n",
    "\n",
    "Simple linear regression is a linear model with a single explanatory variable $x$. The model assumes a linear relationship between the response variable $y$ and the explanatory variable $x$, i.e., \n",
    "\n",
    "$y_{i}=\\beta_0+\\beta_1 x_{i}+\\epsilon_{i}$\n",
    "\n",
    "$x$ : the explanatary variable. $X$ is fixed.\n",
    "\n",
    "$y$: the response variable.\n",
    "\n",
    "$\\epsilon$ : the error term. The model assumes that $\\epsilon_{i}{ }^{\\prime} s$ are iid random variables with the normal $\\left(0, \\sigma^{2}\\right)$ density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7febe393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Girth</th><th scope=col>Height</th><th scope=col>Volume</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> 8.3</td><td>70</td><td>10.3</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 8.6</td><td>65</td><td>10.3</td></tr>\n",
       "\t<tr><th scope=row>3</th><td> 8.8</td><td>63</td><td>10.2</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>10.5</td><td>72</td><td>16.4</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>10.7</td><td>81</td><td>18.8</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>10.8</td><td>83</td><td>19.7</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & Girth & Height & Volume\\\\\n",
       "  & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 &  8.3 & 70 & 10.3\\\\\n",
       "\t2 &  8.6 & 65 & 10.3\\\\\n",
       "\t3 &  8.8 & 63 & 10.2\\\\\n",
       "\t4 & 10.5 & 72 & 16.4\\\\\n",
       "\t5 & 10.7 & 81 & 18.8\\\\\n",
       "\t6 & 10.8 & 83 & 19.7\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 3\n",
       "\n",
       "| <!--/--> | Girth &lt;dbl&gt; | Height &lt;dbl&gt; | Volume &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 |  8.3 | 70 | 10.3 |\n",
       "| 2 |  8.6 | 65 | 10.3 |\n",
       "| 3 |  8.8 | 63 | 10.2 |\n",
       "| 4 | 10.5 | 72 | 16.4 |\n",
       "| 5 | 10.7 | 81 | 18.8 |\n",
       "| 6 | 10.8 | 83 | 19.7 |\n",
       "\n"
      ],
      "text/plain": [
       "  Girth Height Volume\n",
       "1  8.3  70     10.3  \n",
       "2  8.6  65     10.3  \n",
       "3  8.8  63     10.2  \n",
       "4 10.5  72     16.4  \n",
       "5 10.7  81     18.8  \n",
       "6 10.8  83     19.7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(trees)\n",
    "head(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c634d8",
   "metadata": {},
   "source": [
    "The probability distribution of $y_{i}$ is $\\operatorname{Normal}\\left(\\beta_0+\\beta_1 X_{i}, \\sigma^{2}\\right)$. The expectation of $y$ is a linear function of $x$, i.e., $E(y)$ and $x$ has a linear relationship,\n",
    "\n",
    "$$E(y) = \\beta_0+\\beta_1x$$(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed59a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD/AAD///89ODILAAAACXBIWXMAABJ0\nAAASdAHeZh94AAAalklEQVR4nO3djVbqSBaA0QogKCLD+z/tSFDE24qQnNRP2Hut6YvTl64y\n5hNSCZAOwGip9ARgDoQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQE\nAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQE\nAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQE\nAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQE\nAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQE\nAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEAYQEATKElKAxA/by+HAKDAGRhAQBhAQBhAQB\nhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAQBhAS3uvKKciHBba6+\nOYOQ4DZCgvGuv1+QkOAmQoIAQoIIjpEggJAghPNIMC0hQQAhQQAhQQAhQQAhQQAhQQAhQQAh\nQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAh\nQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAhQQAh\nQYCsIb1uVulotX6daggoImNI+0X6spxkCCgkY0jr1L3s+ltv2y6tpxgCCskYUpd259u71E0x\nBBSSMaSUfvsibAgoxCMSBMh7jLR96285RmJuci5/Ly9W7Rb7SYaAMvKeR1r355G61cZ5JObF\nlQ0QoJ6Q0qVphoCp5Fz+7v54Qjd+CCgk63mktLq6xDB+CCgka0jHVe+bUhISjcl7ZcN+ldLT\ndrohoJDclwjtjgvgq+fd9QcmIdGY/Nfa7dbdnwtzQqIxRS5a3T2vFkJiTkpd/T3NEFCIkCBA\nPVc2ZB4CIgkJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJ\nAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJ\nAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJ\nAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAggJAmQN6XWzSker9etUQ0AR\nGUPaL9KX5SRDQCEZQ1qn7mXX33rbdmk9xRBQSMaQurQ7396lboohoJCMIaX02xdhQ0AhHpEg\nQN5jpO1bf8sxEnOTc/l7ebFqt9hPMgSUkfc80ro/j9StNs4jMS+ubIAA9YSULk0zBEwlZ0j7\n9XGpbrNIafky0RBQRsaQ3rr3R5p95xIhZihjSE9ptX//x9Pbe1NPlr+ZlaxXNuw//vH+LM8J\nWWYl9yVCXbr4InwIKCTrU7vd4bA5XSe0v36QJCQakzGkXerWu8Oqey9pu0jbKYaAQnIuf2+7\nrxNFm2mGgDLynpB9eepfJbvavE02BJRQz5UNmYegmFleuSIk8prpRWBCIi8hjbhLhUNQxlyv\nSxYSWQlpzF0qHIIyhDTmLhUOQSHz7EhIZCakEXepcAiKmWFGQoIQQoIAQoIAQoIAQoIAQoIA\nQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQqIJtb/2Qkg0oP5XAwqJevzaipCGqXmLMZXfa2ng\nHVOERC2EFK7iDcZUrtQipIEq3mBM5Vot1XckJGohpHg1bzGmcrWWujMSEvWo/2HnCiFRj2Yz\nEhKEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBIE\nEBIEEBIEEBIEEBIEEBIEEBIEEBIEEBK5Nfx+qr8TEnk1/Q7fvxMSeQlpxF0qHIIyGvjwvUGE\nRFZCGnOXCoegDCGNuUuFQ1DIPDvKG9LrZtVvxNX6daohqJxHpDF36e0X6ctykiGonpDG3KW3\nTt3Lrr/1tu3SeoohqJ6Qxtyl16Xd+fYudVMMQf3m2VHOkL5tvOtbcnabmTMhjbhLzyMSvRlm\nlPsYafvW33KMxNzkXP5eXqzaLfaTDAFl5D2PtO7PI3WrjfNIzIsrGyBAPSGlS9MMAVPJH9Lz\nIqXVdtIhILfs55E+VhyuLtoJidbkDmmd1vvD4W2dnqcYAgrJHVKX+nXvfVpMMQQUkjukz3UE\nlwgxK7lDevoMySVCzEnWkFab5216eb+5X7tEiFnJGtL5HFFKnUuEmJOc55F2u+fn1apfclhf\n7UhItKaeKxsyDwGRhATj/e9/999HSPDNe0YekWCc/tFISDDGx5M6IcFw52MjIcFQF0sMQoJh\nvq3UCQmG+GfBW0hwv/+cNxIS3OuH069Cgvv8eBXD6JC2q+P13Ku3YXO6aQioxy8XA40NaXl6\nYUTqQksSEnX69Zq6kSE9p+X+GNJzehoyrVuGgFpcuTR1ZEjH9zL59l4MMYREfa5e4T0ypP5p\nnZCYvz9eKDEypMXHI9Lu+ttrjRkCyvvz9UYxx0jb7vobPo4ZAkq74WV7Y1ftVjd9SvmoIaCo\nm179GnIeKa1e7v/P3D4ElHPji8hd2QC/u/m9GIQEv7njLU2EBD+7652Bxoa07qb4kD0hUdqd\nb7A1MqT1NJ9WKSTKuvt96kZf2RB6/uinISC3AW/3GHCJ0ASERDlD3jV1/FO76++GP5CQKGVQ\nRgGvR1rGvqTvhyEgm4EZjQ9pa7GB2Ric0eiQNlbtmIsRGQW8sM+qHbMwKiOrdnA0MqOAp3ZW\n7Wje6IzGLzZslq+j5/DHEDCpgIwCntpZbKBpIRkJiccWlJGXUfDIwjISEo8rMCMh8ahCM3KM\nxGMKzkhIPKLwjKKe2r0uV+Oncn0ICDJBRmHHSHufRkEbJskobrHBUztaMFFGYSE9p270VP4Y\nAsaaLKPAxYZN2JQOQmIKE2YUFtIi9mVJQiLapBk5Icvfgs9tFDFxRkLiLxOcJsxu8oxGhZS+\nKzwrJtJ+SBkyEhJ/mOTnm1OWjDy14w+Nh5QpIyHxh6ZDypZRQEgvy+SjL+es3Y4yZhTwlsUf\n29mHMc9VqyFlzWh0SM+p277/sQ1+o8j2fmxzJqMbjAxpkXb9n7u0iJnPf4eAe2XPKO6dVi1/\nU4sCGQU+Irn6myoUycgxEvNSKCOrdsxJsYwiziOtnEeiCgUzGhXSNnQiPw4Btyqa0biLVrv1\nnZ8f+7pZ9c8DV+s/PsJCSNyncEajQlocD43ueFjaLy6uFb9+TCUk7lE8o3HHSG/r7j2J9e7G\n+61T93L6u2/bLq2DZ8XDqiCj0YsNr0/9Gzbc9LF9XfpK7o/zTkLiVlVkFPEyiv7y76cbnuJ9\nu/jh+pUQQuI2lWQU83qk/eb96OfvKxs8IhGsmozCXti3veFau/djpO1pmc8xEgEqyijrI9L5\nKoj+jfCuHlYJib9UlVHWY6TD4XXdn0fqVhvnkRilsoxGh7S9Y9Vu4BDwr+oyGhfS6/E8Unfz\neaS//rNTvbcXM1NhRlmvbLj4L/w1qpD4TZUZjbzWbjPwKZ2QGKjSjEaF9MeCwX/vd/s7swqJ\nn1SbUdY3iHzthMQIFWeU951W96u07M/IemrH3arOKPdbFr+kdHwtrZC4U+UZZX/v77dlWu2F\nxH2qz6jAm+hvUvf3hXlC4ksDGZX4NIrd4u8TrkLiUxMZlflYlychcaNGMvL5SHV79IulmslI\nSDV79OsOG8pISDV77JCaykhIFXvoS+Eby0hIFXvgkJrLSEgVe9iQGsxISDV7zI6azEhINXvE\nkBrNSEh1k1EzhEQtGs5ISNSi6YyERB0az0hI1KD5jIREeTPISEiUNouMhERZM8lISJQ0m4yE\nNEetnMadUUZCmp9WLiyaVUZCmp82QppZRkKanSZefDG7jIQ0Ow2ENMOMhDQ71Yc0y4yEND91\ndzTTjIQ0PzWHNNuMhDRHMipASOQx64yERB4zz0hI5DD7jITE9B4gIyExsf89REZCYlIPUtFB\nSEzocTISEmNcPWP1SBkJieGuXkPxWBkJieGuhPRoGQmJwX6/zvzxMhISg/0W0iNmJCQG+zmk\nx8xISAz3Q0ftZBR9ibyQGOo/ITWVUfCLtoTEcI1mJCSq1VJGU7yxhZAI0FRGQqJOjWUkJGrU\nXEYHx0hUp8WMhERl2szoyHkkqtFuRvGExEAyuiQkBpHRd0JiABn9S0jcTUb/JSTuJKOfCIm7\nyOhnQuIOMvqNkLiZjH4nJG4UllGtn980ipC4SWRG1X6i4AhC4gaBT+qENOIuFQ7B7SKPjar/\n1PWBhMQfYpcYhDTmLhUOwW2iV+qENOYuFQ7BLSZY8J5nR0Lid5OcNxLSiLt8eN2s+o24Wr9O\nNQRhJjv9OsOMsoa0X6Qvy0mGIIyrGO6TMaR16l52/a23bZfWUwxBEBndK2NIXdqdb+9SN8UQ\nhJDR/TKG9O2Z8fWnyUIqSEZDeETiGxkNk/cYafvW33KMVCsZDZVz+Xt5sWq32E8yBGPIaLi8\n55HW/XmkbrVxHqk+MhrDlQ30ZDROPSGlS9MMwW9kNFaBkJ67tHiedgjuIqPxcoa0W6Xu+bBx\niVBdZBQhY0i7vqB1etof3lbp6mOSkLKRUYyMIT0dzx2tT2di92kxxRDcSUZRsl8ilFYXX0QP\nwV1kFCd7SC+n53QuESpORpGyPrV7+rycYf/kEqHCZBQr5wv7uvPzuXT9AUlIU5NRtKznkdaf\n+XRXH4+ENDEZxavnyobMQzwuGU1BSA9GRtMQ0kOR0VSE9EBkNB0hPQwZTUlID0JG0xLSQ5DR\n1IT0AGQ0PSHNnoxyENLMyehug97qQEizJqO7DXzXECHNmIwGEBLfyWiIoW9kJaSZktEwQuKC\njIYSEmcyGsExEicyGkVIHMloNOeRkFEpQpoRGZUjpNmQUUlCmgkZlSWkWZBRaUIqb/QHq8mo\nPCGVNvozCmVUAyGVNjIkGdVBSIWN+9hcGdVCSIWNCUlG9RBSYcNDklFNhFTawI5kVJfZhDR6\nDbmUQSHJqDYzCWn0GnJJMpoBITVHRjWaR0jj1pCbIqM6CakpMqqVkBoio3rNI6SHOEZqK6O5\n/zT+JaRGNJfRzH8e/5pJSHP/DdhWRkKa6i4VDtGS1jJ6lGPWS0KqXnMZCWmyu1Q4RCsazEhI\nk92lwiHa0GRGB8dIU92lwiFa0GpGQprqLhUOUb92Mzp6rIyEVK22M3o8QqqSjFojpArJqD1C\nqo6MWiSkysioTUKqykNkNMsFPSFV5FEymuMpJiFV4yEyEtKou1Q4RG0CM6p6N53rZXhCqkJs\nRjXvp0Iac5cKh6hJ6JO6yvdTIY25S4VD1CP22Kj6HbXy6Q0lpMKilxiEVIaQiopfqas+pMoX\nQ4YSUjn/m2TBu/qO5klIpUx12mhgSNobR0iRbt8bpzz7OiwjD2OjCCnO7XtjdRcxCGksIcW5\ndW+sLqMWVihqJ6QwN+6N9WUkpABCCnPT3lhjRkIKkDWk182q/3Gt1q9TDVHQDXtjnRkJKUDG\nkPaL9GU5yRBl/bUz1pqRkAJkDGmdupddf+tt26X1FEOUdX1nrDcjIQXIGFKXdufbu9RNMURp\nbWZ0sPw9XsaQvv2crv/Q5vYTrTwjIY3nEWl61Wd0JKNx8h4jbd/6WzM9RvpZExkxVs7l7+XF\nqt1iP8kQ1akrIw87k8l7Hmndn0fqVps5nkf6QXUZORCaiisbplNXRvMNqYrvqZ6Q0qVphsiq\ntozmerKokm8qZ0j7p5SW24//yMyXv6vLSEgTTyPLXXr77nSh3ek/MuuQKsxoriHV8l1lXf5+\nfq/puesvs5tzSFVmdKjmd3esBwypO93xrVu8zTmkWjMS0rTzyHKX0/0+7rhfLucbUr0ZzTSk\nWr6rjCEt0udJ2MVypiHVnFE1u9wVQ2ZXyXeVMaTn9PRx6y0t5xhS3RlV8yToV0OnV8W3lHP5\ne33+hrd/fO8VbJi7VZ7RjEOqQtYTsrvV5623p5mFVH1G9YdU+/yuq+fKhsxDhGogo0P1v/GF\nFK+tbdlGRkKalJDGaiWjo7r30pY7EtJIjWVU9Y5a+/yuEtIYLWXUwo5a9+yuEtJwbWXU+DFI\n7YQ0VGMZCWlaQhqmuYyENC0hDdFgRocWjpEaJqT7tZmRkCYlpHu1mtGRjCYjpPu0nBETEtI9\nZMQvhHQ7GfErId1KRlwhpNvIiKuEdAsZ8Qch/a1cRparmyGkvxTNyAnUVgjpupJP6oTUECFd\nU/TYyEWmLRHS7wovMQipJUL6TfGVOiG1REg/K57RwTFSUx47pN920xoyElJTHjmk33bUOjI6\nklEzhPTvWPVkREMeOKQfD+ZlxCBCugxJRgwkpK+QZMRgDxzSP8dIMmIEIZ3GkhGjPHJIX8vL\nMmKkxw7pREaMJiQZEeDRQ5IRIR47JBkR5JFDkhFhHjckGRHoUUOSEaEeMyQZEewRQ5IR4R4v\nJBkxgUcLSUZM4rFCkhETeaSQZMRkHickGTGhRwlJRkzqMUKSERN7hJBkxOTmH5KMyGDuIcmI\nLOYdkozIZM4hyYhs5huSjMhoriHJiKzmGZKMyGyOIcmI7OYXkowoYG4hyYgi5hWSjChkTiHJ\niGLmE5KMKGguIcmIouYRkowobA4hyYjisob0uln1Hza5Wr/GDSEjKpAxpP0ifVkGDSEjqpAx\npHXqXnb9rbdtl9YRQ8iISmQMqUu78+1d6sYPcWtGnx+5XFQVk2AyGUP6tidd361++5eXe+Md\nGaXie3EVk2BCLT0iXe6Ntz+pq2IfrmISTCjvMdL2rb818Bjpa2+849jovLxx+0TjVTEJppRz\n+Xt5sWq32N89xPm+dy0xVLEPVzEJppT3PNK6P4/UrTZDziMNyaiSfbiKSTClhq5s+Mjo3r2x\nil24ikkwoXpCSpd++RvHjO4dvop9uIpJMKF6Qvp7iD6jAXtjFXtwFZNgMu2EdDw2sjdSqVZC\ncjEQVct6ZcPfh0G/DCEjKpcxpOehIcmI6uV8arfrrr944uchZEQDsh4j7a5fGPTTEDKiCXkX\nG54vrlu9ZQgZ0YiaV+1kRDPqDUlGNKTWkGREU+oMSUY0ps6QXAhEY4QEAYQEAYQEAYQEAYQE\nAYQEAYQEASoNCRozYC+PD2diVczYJM6qmEX5SZSfwb2qmLFJnFUxi/KTKD+De1UxY5M4q2IW\n5SdRfgb3qmLGJnFWxSzKT6L8DO5VxYxN4qyKWZSfRPkZ3KuKGZvEWRWzKD+J8jO4VxUzNomz\nKmZRfhLlZ3CvKmZsEmdVzKL8JMrP4F5VzNgkzqqYRflJlJ/BvaqYsUmcVTGL8pMoP4N7VTFj\nkzirYhblJ1F+BveqYsYmcVbFLMpPovwMYAaEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGE\nBAGEBAGEBAGEBAGEBAGEBAHaCWn3lNLT2/HW4Dc6H+vbe6yvu9St92UnUWxLHA77i++/1Ka4\nnETBTdFrJqRtv5269422Kx5S93572d9aFJ1EuS1xeOtOkzj+Ziu2KS4mUXBTnDQTUtftDvtV\nWh+32aroTLbp9XB4Te/z2XXHm+UmUXBLPB1/Eod1eiq5KS4mUXqnaCakl36b7Y+/h5/TpuRM\n9t3xR7ZO235WhaZymkTBLfHxu//4R7lNcTGJwjtFOyE9pd3nzef0XHImq7Tv/3l8UlPs9+Bp\nEgW3RPexD3clN8XFJArvFO2EtEiHTZeeTvvw9un9ELPQRHb9Q+PlL8Nykyi4JTYfz6o2JTfF\nxSTK7hSHdkJKafV5mL86HVYuy0zk9FhQOKSPSZTcEs/HA/3u+DBQcFN8TaLsTnFoKaTjYsPT\n6Tfgy3Hhs8xj+e54ZHsoHNLXJMptiU2/424ORTfF5SQK7hRH7YR0PEZ6+1pk3ZdYb/08si4c\n0uckTopsiefjs6r3X2zPJTfFxSROCu0UR+2EdPnHPzcz+ji+PR/nFp3EhxKTWPRPLvsdt9ym\nuJjEh3InkloJaVVHSOe1qdNS1VuRVbt/F8jKnJs+/1FuU9Ty27UfudTAd9r0z2bejkeTXf97\nqMw+fF5lPc1nm0qsE50nUXBLnB6G+vN65TbFxSRK7hS9VkJ6PzraH58OvxyPENb9ceX273uF\nW32ezip5ZcN5EgW3xPvQ+48JlNsUF5MouVP0WgnpY4HmuLy5P11hVeScwelZ+elWseXW8yRK\nbonl1/dfblMsq9gpes2EdNguP0+4Ha/5XZRZ5/x6Dn668LiCSRTaEoev77/gpvg+iVKb4qid\nkKBiQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIA\nQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQoIAQqrc+eP5/vnA\n7t+/LPYxqg9NSJW7O6SFH2kJtnrlfgvpt7/2x99jIrZ65YTUBlu9cv+E9LxI3fPXl+surfub\n7/9bp25zvJGkVIBtXrnvIa36TpafXy6PXz2dQur/1bOQCrHNK5e+HA7btNwf9su0PYW0Td3u\nsOtOIb3/m+e08NSuEFu9ct9CWqX9+/+1T6tTL6tjUe859SG9Hg6fT/LIz1av3LendhdNXTTz\ndVNIxdjqlRNSG2z1yv0T0rf/X0j1sNUr9y2k00HRP19uhVQBW71y30J6OS7THZ4/Fxu+rdp9\n/p2U3gpO92EJqXLfzyP1J45S9/bty+8hLd7/fbHZPi4hVe6/Vzakp7fzl+suLV+/h/S6EFIB\nQmpff6UDZQmpYSm9HA77VVqXnghCatnmdITkmVwFhNSy52VKC49HNRASBBASBBASBBASBBAS\nBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBAS\nBBASBBASBBASBBASBPg/xUSn4p8jN+MAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "D:\\OneDrive\\Github\\Jupyter\\jupyter_book\\binf8441\\_build\\jupyter_execute\\chap9_3_0.png"
      },
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(Volume ~ Height, data = trees, pch=16)\n",
    "abline(lm(Volume ~ Height , data=trees), col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43322957",
   "metadata": {},
   "source": [
    "The intercept $\\beta_0$ and slope $\\beta_1$ are two parameters to estimate from data. Given $\\beta_0$ and $\\beta_1$, the equation (1) can be used to predict the value of $y$ for a new value of $x$, i.e., \n",
    "\n",
    "$$y_{new} = \\beta_0+\\beta_1x_{new}$$(2)\n",
    "\n",
    "\n",
    "### 1. Least square estimates\n",
    "\n",
    "We estimate $\\beta_0$ and $\\beta_1$ by minimizing the sum of squared errors with respect to $\\beta_0$ and $\\beta_1$,\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}\n",
    "$$\n",
    "\n",
    "Taking the derivative of the sum of squared errors with respect to $\\beta_0$ and $\\beta_1$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}}{\\partial \\beta_0}=0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}}{\\partial \\beta_1}=0\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\hat{\\beta_0}=\\bar{y}-\\beta_1 \\bar{x}\\\\\n",
    "\\hat{\\beta_1}=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\\\\n",
    "\\widehat{\\sigma^{2}}=\\frac{\\sum\\left(y_{i}-\\hat{y}\\right)^{2}}{n-2}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403e238c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Volume ~ Height, data = trees)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)       Height  \n",
       "    -87.124        1.543  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "-87.1236135388527"
      ],
      "text/latex": [
       "-87.1236135388527"
      ],
      "text/markdown": [
       "-87.1236135388527"
      ],
      "text/plain": [
       "[1] -87.12361"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1.54334975369458"
      ],
      "text/latex": [
       "1.54334975369458"
      ],
      "text/markdown": [
       "1.54334975369458"
      ],
      "text/plain": [
       "[1] 1.54335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm(Volume ~ Height , data=trees)\n",
    "\n",
    "x = trees$Height\n",
    "y = trees$Volume\n",
    "\n",
    "beta1 = sum((x-mean(x))*(y-mean(y)))/sum((x-mean(x))^2)\n",
    "beta0 = mean(y) - beta1*mean(x)\n",
    "\n",
    "beta0\n",
    "beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37d52f",
   "metadata": {},
   "source": [
    "### 2. Maximum likelihood estimates \n",
    "\n",
    "We first find the likelihood function\n",
    "\n",
    "$$\n",
    "f\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right)=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{\\sum\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}}{2 \\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "Next, we find the log-likelihood\n",
    "\n",
    "$$\n",
    "n \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)-\\frac{\\sum\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}}{2 \\sigma^{2}}\n",
    "$$\n",
    "\n",
    "The log-likelihood is maximized if $\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}$ is minimized. Thus, the maximum likelihood estimates are identical with the least square estimates of $\\beta_0$ and $\\beta_1$.\n",
    "\n",
    "### 3. Testing the linear relationship\n",
    "```{important}\n",
    "If $\\beta_1=0$, we say that $Y$ and $X$ do not have the linear relationship\n",
    "\n",
    "If $\\beta_1>0$, we say that $\\mathrm{Y}$ and $\\mathrm{X}$ are positively correlated\n",
    "\n",
    "If $\\beta_1<0$, we say that $\\mathrm{Y}$ and $\\mathrm{X}$ are negatively correlated \n",
    "```\n",
    "\n",
    "The likelihood ratio test (LRT) can be used to test if $Y$ and $X$ have a linear relationship.\n",
    "\n",
    "$\\mathrm{H}_{0}: \\beta_1=0$ and $\\mathrm{H}_{1}: \\beta_1 \\neq 0$\n",
    "\n",
    "The test statistic is $t=2 \\log \\left(l_{1}\\right)-2 \\log \\left(l_{0}\\right)$, where $\\log \\left(l_{0}\\right)$ is the loglikelihood score of the null model\n",
    "\n",
    "$$\\log \\left(l_{0}\\right)=n \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)-\\frac{\\sum\\left(y_{i}-\\beta_0\\right)^{2}}{2 \\sigma^{2}}=n \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\hat{\\sigma}^{2}}}\\right)-\\frac{\\sum\\left(y_{i}-\\hat{\\beta_0}\\right)^{2}}{2 \\hat{\\sigma}^{2}}$$\n",
    "\n",
    "$\\log \\left(l_{1}\\right)$ is the loglikelihood score of the alternative model, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\log \\left(l_{1}\\right) &=n \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)-\\frac{\\sum\\left(y_{i}-\\left(\\beta_0+\\beta_1 x_{i}\\right)\\right)^{2}}{2 \\sigma^{2}} \\\\\n",
    "&=n \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\hat{\\sigma}^{2}}}\\right)-\\frac{\\sum\\left(y_{i}-\\left(\\hat{\\beta_0}+\\hat{\\beta_1} x_{i}\\right)\\right)^{2}}{2 \\hat{\\sigma}^{2}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\mathrm{H}_{0}$ has one free parameter $\\beta_0$, while $\\mathrm{H}_{1}$ has two free parameters $\\beta_0$ and $\\beta_1$. Thus, the null distribution of the test statistic $t$ is the chi-square distribution with 1 degree of freedom.\n",
    "\n",
    "Rejection region: we reject the null if $t>a$ where $a$ is the 95% quantile of the chi-square distribution with 1 degree of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9148f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Volume ~ Height, data = trees)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-21.274  -9.894  -2.894  12.068  29.852 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -87.1236    29.2731  -2.976 0.005835 ** \n",
       "Height        1.5433     0.3839   4.021 0.000378 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 13.4 on 29 degrees of freedom\n",
       "Multiple R-squared:  0.3579,\tAdjusted R-squared:  0.3358 \n",
       "F-statistic: 16.16 on 1 and 29 DF,  p-value: 0.0003784\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = lm(Volume ~ Height, data=trees)\n",
    "summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd7014",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "$y_{i}=\\beta_{0}+\\beta_{1} x_{1 i}+\\beta_{2} x_{2 i}+\\cdots+\\beta_{p} x_{p i}+\\varepsilon_{i}$\n",
    "\n",
    "$x_{i}$ : explanatary variables. We assume $X_{i}$ is fixed.\n",
    "\n",
    "$y$ : the response variable. random variable\n",
    "\n",
    "$\\epsilon$ : the error term. The model assumes that $\\epsilon_{i}^{\\prime} s$ are independent and have the same probability distribution normal $\\left(0, \\sigma^{2}\\right)$.\n",
    "\n",
    "The probability distribution of $y_i$ is normal with mean $\\left(\\beta_{0}+\\beta_{1} X_{1 i}+\\beta_{2} X_{2 i}+\\cdots+\\right.$ $\\left.\\beta_{p} X_{p i}\\right)$ and the variance of $\\mathrm{Y}_{\\mathrm{i}}$ is $\\sigma^{2}$, in which $\\left(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\right)$ and $\\sigma^{2}$ are parameters to estimate.\n",
    "\n",
    "The matrix representation of multiple linear regression with $p$ predictors $\\left(X_{1}, \\ldots, X_{p}\\right)$ is\n",
    "\n",
    "$$\n",
    "Y=X \\beta+\\epsilon\n",
    "$$\n",
    "\n",
    "where $\\mathrm{Y}$ is a vector of size $n$, $\\mathrm{X}$ is a $n \\times(p+1)$ matrix, and $\\beta=\\left(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\right)$ is a vector of size $(p+1)$.\n",
    "\n",
    "### 1. Least square estimates of $\\beta$ \n",
    "We estimate $\\beta=\\left(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\right)$ by minimizing the sum of squared errors\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{1 i}+\\beta_{2} x_{2 i}+\\cdots+\\beta_{p} x_{p i}\\right)\\right)^{2}\n",
    "$$\n",
    "\n",
    "### 2. Maximum likelihood estimates\n",
    "The likelihood function is given by \n",
    "\n",
    "$$\n",
    "f\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right)=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{1 i}+\\beta_{2} x_{2 i}+\\cdots+\\beta_{p} x_{p i}\\right)\\right)^{2}}{2 \\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "Since maximizing the likelihood function is equivalent to minimizing $\\left(y_{i}-\\left(\\beta_{0}+\\right.\\right.$ $\\left.\\left.\\beta_{1} X_{1 i}+\\beta_{2} X_{2 i}+\\cdots+\\beta_{p} X_{p i}\\right)\\right)^{2}$, the MLEs of $\\beta$ are identical with the least square estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e7a7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Volume ~ Girth + Height, data = trees)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\n",
       "Girth         4.7082     0.2643  17.816  < 2e-16 ***\n",
       "Height        0.3393     0.1302   2.607   0.0145 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 3.882 on 28 degrees of freedom\n",
       "Multiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \n",
       "F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = lm(Volume ~ Girth + Height, data = trees)\n",
    "summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab8060",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "\n",
    "The response variable $\\mathrm{Y}$ may not have a linear relationship with $\\mathrm{X}$. For example, if the true relation is $E(Y)=\\beta_0+\\beta_1 X^{2}$, we can transform $Z=X^{2}$ and fit a linear model for $\\mathrm{Y}$ and $\\mathrm{Z}$ (or $\\mathrm{X}^{2}$ )\n",
    "\n",
    "In general, if $E(Y)=g(X)$, we can fit a linear model for $Y$ and a function $g(X)$ of $X$, where the function $g(X)$ can be found by\n",
    "\n",
    "1) The residual plots to discover the relationship between $\\mathrm{Y}$ and $\\mathrm{X}$ and find $g(X)$\n",
    "\n",
    "2) Log-transformation for $\\mathrm{X}$ and $\\mathrm{Y}$\n",
    "\n",
    "3) Box-Cox transformation: $Y=\\left\\{\\begin{array}{l}\\frac{Y^{\\lambda}-1}{\\lambda} \\text {, if } \\lambda \\neq 0 \\\\ \\log (Y) \\text {, if } \\lambda=0\\end{array}\\right.$ and find the optimal $\\lambda$ using $\\mathrm{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523a015b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "D:\\OneDrive\\Github\\Jupyter\\jupyter_book\\binf8441\\_build\\jupyter_execute\\chap9_11_0.png"
      },
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "\n",
    "boxcox (Volume ~ log(Height)+log(Girth), data = trees, lambda =seq(-0.25,0.25, length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373365f5",
   "metadata": {},
   "source": [
    "## Variable selection\n",
    "\n",
    "Variable selection is intended to select the \"best\" subset of predictors. We want to explain the data in the simplest way. The principle is that among several plausible explanations for a phenomenon, the simplest is best. Applied to regression analysis, this implies that the smallest model that fits the data is best.\n",
    "\n",
    "Considering the linear regression model with $p$ predictors $\\left\\{X_{1}, \\ldots, X_{p}\\right\\}$\n",
    "\n",
    "$$\n",
    "Y=\\beta_{0}+\\beta_{1} X_{1}+\\cdots+\\beta_{p} X_{p}+\\epsilon\n",
    "$$\n",
    "\n",
    "We want to select the smallest subset of predictors $\\left\\{X_{1}, \\ldots, X_{p}\\right\\}$ that can fit the data, i.e., we are interested in testing\n",
    "\n",
    "$$\n",
    "H_{0}: \\beta_{j}=0 \\text { vs } H_{1}: \\beta_{j} \\neq 0\n",
    "$$\n",
    "\n",
    "With the normality assumption, we may use t-test to test whether $\\beta_{j}=0$. In general, to select a subset of predictors, we need to test if a subset $\\beta_{s}$ of $\\beta$ are 0 , i.e.,\n",
    "\n",
    "$$\n",
    "H_{0}: \\beta_{s}=0 \\text { vs } H_{1}: \\beta_{s} \\neq 0\n",
    "$$\n",
    "\n",
    "The model under $H_{0}$ is called reduced model with $p_{s}+1$ coefficients $\\beta$ and the model under $H_{1}$ is called full model with $(p+1)$ coefficients $\\beta$. Note that $p_{s}<p$, and the null model is nested in the alternative model.\n",
    "\n",
    "### 1. F-test \n",
    "\n",
    "The residual sum-of-squares $\\operatorname{RSS}(\\beta)$ is defined as:\n",
    "\n",
    "$$\n",
    "R S S(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}-\\widehat{y}_{l}\\right)^{2}=\\sum_{i=1}^{n}\\left(y_{i}-x_{i} \\beta\\right)^{2}\n",
    "$$\n",
    "\n",
    "Let $\\mathrm{RSS}_{1}$ correspond to the full model with $p+1$ parameters, and $\\mathrm{RSS}_{0}$ correspond to the nested model with $p_{0}+1$ parameters. The $\\mathrm{F}$ statistic measures the reduction of RSS per additional parameter in the full model,\n",
    "\n",
    "$$\n",
    "F=\\frac{\\left(R S S_{0}-R S S_{1}\\right) /\\left(p_{1}-p_{0}\\right)}{R S S_{1} /\\left(n-p_{1}-1\\right)}\n",
    "$$\n",
    "\n",
    "Under the normal assumption, the null distribution of $\\mathrm{F}$ test statistic is the $\\mathrm{F}$ distribution with degrees of freedom $\\left(p_{1}-p_{0}\\right)$ and $\\left(n-p_{1}-1\\right)$. Thus, we reject $\\mathrm{H}_{0}$ if $F>$ a, where $\\mathrm{a}$ is the $(1-\\alpha)$ quantile of the $\\mathrm{F}$ distribution with degrees of freedom $\\left(p_{1}-p_{0}\\right)$ and $\\left(n-p_{1}-1\\right)$.\n",
    "\n",
    "### 2. Likelihood ratio test (LRT)\n",
    "\n",
    "Let $\\mathrm{L}_{1}$ be the maximum value of the likelihood of the full model. Let $\\mathrm{L}_{0}$ be the maximum value of the likelihood of the nested model. The likelihood ratio test statistic is\n",
    "\n",
    "$$\n",
    "t=2 \\log \\left(L_{1}\\right)-2 \\log \\left(L_{0}\\right)\n",
    "$$\n",
    "\n",
    "The null distribution of test statistic $t$ is asymptotically $\\chi^{2}$ distribution with $\\left(p_{1}-p_{0}\\right)$ degrees of freedom where $p_1$ is the number of parameters in the alternative model and $p_0$ is the number of parameters in the null model. Thus, we reject $\\mathrm{H}_{0}$ if $t>\\mathrm{a}$, where $a$ is the $(1-\\alpha)$ quantile of the $\\chi^{2}$ distribution with $\\left(p_{1}-p_{0}\\right)$ degrees of freedom.\n",
    "\n",
    "### 3. Akaike Information Criterion (AIC)\n",
    "\n",
    "The LRT can only be applied to the nested models. In addition, the LRT tends to favor the complex model, because LRT is solely based on the likelihood score and the complex model always has a higher likelihood score. AIC is a more general measure of \"model fit\" by penalizing the complexity of the model to avoid overfitting the data,\n",
    "\n",
    "$$\n",
    "\\text { AIC }=-2 \\text { loglikelihood }+2 p\n",
    "$$\n",
    "\n",
    "where $p$ is the number of parameters, measuring the complexity of the model, and loglikelihood measures the goodness of fit of the model to the data. Given a collection of putative models, the best model is the one with the lowest $A I C$.\n",
    "\n",
    "### 4. Bayes Information Criterion (BIC)\n",
    "\n",
    "AIC tends to overfit models when the sample size is small. Another information criterion which penalizes complex models more severely is \n",
    "\n",
    "$$\n",
    "\\text { BIC }=-2 \\text { loglikelihood }+p * \\log (n)\n",
    "$$\n",
    "\n",
    "Given a collection of putative models, the best model is the one with the lowest BIC.\n",
    "\n",
    "### 5. Algorithms for variable selection\n",
    "\n",
    "An exhaustive search for the subset may not be feasible if $p$ is very large. There are two main alternatives:\n",
    "\n",
    "#### Forward stepwise selection\n",
    "The algorithm begins with a naive model $y = \\beta_0$ that does not include any explanatary variables $x$. Then, we add one explanatary variable at a time. We always choose from the rest of the variables the one that yields the best accuracy in prediction when added to the pool of already selected variables. This accuracy can be measured by the F statistic, LRT, AIC, BIC, etc. The algorithm stops when no predictors can be added.\n",
    "\n",
    "#### Backward stepwise selection\n",
    "The algorithm begins with the full model including all predictors and then removes one predictor at a time.\n",
    "\n",
    "#### Forward and backward stepwise selection\n",
    "mixing forward and backward selection to find the optimal subset of predictors.\n",
    "\n",
    "#### Least absolute shrinkage and selection operator (LASSO)\n",
    "The algorithm minimizes the residual sum-of-square $\\operatorname{RSS}(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}-\\widehat{y}_{l}\\right)^{2}$, subject to $\\sum_{i=1}^{p}\\left|\\beta_{i}\\right| \\leq c$, in which $\\mathrm{c}$ is a pre-specified parameter. This procedure can automatically shrink some $\\beta$ to 0 .\n",
    "\n",
    "## Generalized linear regression\n",
    "\n",
    "In linear regression model, the dependent variable $\\mathrm{Y}$ is assumed to have a normal distribution with mean $X \\beta$. In generalized linear regression, the normality assumption is relaxed to allow $Y$ to have other probability distributions (binomial, Poisson, etc).\n",
    "\n",
    "### 1. Logistic model\n",
    "\n",
    "$Y_{i}=(0,1)$ is a Bernoulli random variable with probability $p_{i}$ satisfying $\\log \\left(\\frac{p_{i}}{1-p_{i}}\\right)=X_{i} \\beta$, which is called the link function. It indicates that $p_{i}=\\frac{e^{X_{i} \\beta}}{1+e^{X_{i} \\beta}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7ff109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>admit</th><th scope=col>gre</th><th scope=col>gpa</th><th scope=col>rank</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>380</td><td>3.61</td><td>3</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td>660</td><td>3.67</td><td>3</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>1</td><td>800</td><td>4.00</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1</td><td>640</td><td>3.19</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>520</td><td>2.93</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1</td><td>760</td><td>3.00</td><td>2</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 4\n",
       "\\begin{tabular}{r|llll}\n",
       "  & admit & gre & gpa & rank\\\\\n",
       "  & <int> & <int> & <dbl> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & 0 & 380 & 3.61 & 3\\\\\n",
       "\t2 & 1 & 660 & 3.67 & 3\\\\\n",
       "\t3 & 1 & 800 & 4.00 & 1\\\\\n",
       "\t4 & 1 & 640 & 3.19 & 4\\\\\n",
       "\t5 & 0 & 520 & 2.93 & 4\\\\\n",
       "\t6 & 1 & 760 & 3.00 & 2\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 4\n",
       "\n",
       "| <!--/--> | admit &lt;int&gt; | gre &lt;int&gt; | gpa &lt;dbl&gt; | rank &lt;int&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 1 | 0 | 380 | 3.61 | 3 |\n",
       "| 2 | 1 | 660 | 3.67 | 3 |\n",
       "| 3 | 1 | 800 | 4.00 | 1 |\n",
       "| 4 | 1 | 640 | 3.19 | 4 |\n",
       "| 5 | 0 | 520 | 2.93 | 4 |\n",
       "| 6 | 1 | 760 | 3.00 | 2 |\n",
       "\n"
      ],
      "text/plain": [
       "  admit gre gpa  rank\n",
       "1 0     380 3.61 3   \n",
       "2 1     660 3.67 3   \n",
       "3 1     800 4.00 1   \n",
       "4 1     640 3.19 4   \n",
       "5 0     520 2.93 4   \n",
       "6 1     760 3.00 2   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- read.csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997ccd2",
   "metadata": {},
   "source": [
    "Since $Y_{i}$ 's are independent, the likelihood function is given by\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^{n} p_{i}^{Y_{i}}\\left(1-p_{i}\\right)^{1-Y_{i}}=\\prod_{i=1}^{n}\\left(\\frac{e^{X_{i} \\beta}}{1+e^{X_{i} \\beta}}\\right)^{Y_{i}}\\left(\\frac{1}{1+e^{X_{i} \\beta}}\\right)^{1-Y_{i}}\n",
    "$$\n",
    "\n",
    "The MLE of $\\beta$ is given by maximizing this likelihood function with respect to $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2940b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = admit ~ gre + gpa + rank, family = \"binomial\", \n",
       "    data = data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-1.5802  -0.8848  -0.6382   1.1575   2.1732  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -3.449548   1.132846  -3.045  0.00233 ** \n",
       "gre          0.002294   0.001092   2.101  0.03564 *  \n",
       "gpa          0.777014   0.327484   2.373  0.01766 *  \n",
       "rank        -0.560031   0.127137  -4.405 1.06e-05 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 499.98  on 399  degrees of freedom\n",
       "Residual deviance: 459.44  on 396  degrees of freedom\n",
       "AIC: 467.44\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result <- glm(admit ~ gre + gpa + rank, data = data, family = \"binomial\")\n",
    "summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d9dc5",
   "metadata": {},
   "source": [
    "We can predict the admission probability for a new observation $X$ using the equation $p=\\frac{e^{X \\beta}}{1+e^{X \\beta}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d7e9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>1:</strong> 0.592475691285015"
      ],
      "text/latex": [
       "\\textbf{1:} 0.592475691285015"
      ],
      "text/markdown": [
       "**1:** 0.592475691285015"
      ],
      "text/plain": [
       "        1 \n",
       "0.5924757 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "newdata1 <- data.frame(gre = 590, gpa = 3.9, rank = 1)\n",
    "predict(result, newdata = newdata1, type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e7876",
   "metadata": {},
   "source": [
    "### 2. Loglinear model (counts)\n",
    "\n",
    "$Y_{i}$ is a Poisson random variable with $\\log \\left(E\\left(Y_{i}\\right)\\right)=X_{i} \\beta$. It indicates that $Y_{\\mathrm{i}}$ is a Poisson random variable with mean $\\lambda_{i}=e^{X_{i} \\beta}$. Thus, the likelihood function is given by\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^{n} \\frac{e^{-\\lambda_{i}} \\lambda_{i}^{Y_{i}}}{Y_{i} !}=\\prod_{i=1}^{n} \\frac{e^{-e^{X_{i} \\beta}}\\left(e^{X_{i} \\beta}\\right)^{Y_{i}}}{Y_{i} !}\n",
    "$$\n",
    "\n",
    "The MLE of $\\beta$ are obtained by maximizing the likelihood function.\n",
    "\n",
    "We may use LRT to test $\\mathrm{H}_{0}: \\beta_{1}=0$ vs $\\mathrm{H}_{1}: \\beta_{1} \\neq 0$ for the logistic and loglinear models. The procedures of variable selection can also apply to the logistic and loglinear models."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "source_map": [
   14,
   40,
   43,
   49,
   52,
   91,
   102,
   153,
   156,
   194,
   197,
   211,
   215,
   311,
   314,
   324,
   327,
   331,
   334
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}