

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 12: Bayesian analysis &#8212; Statistical Inference in Bioinformatics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chap12';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 13: Markov Chains" href="chap13.html" />
    <link rel="prev" title="Chapter 11: Bootstrap" href="chap11.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Table of Contents                              
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chap0.html">Chapter 0: Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap1.html">Chapter 1: Probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap2.html">Chapter 2: Discrete random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap3.html">Chapter 3: Continuous random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap4.html">Chapter 4: Multiple random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap5.html">Chapter 5: Estimation theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap6.html">Chapter 6: Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap7.html">Chapter 7: Multiple tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap8.html">Chapter 8: RNA-seq analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap9.html">Chapter 9: Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">Chapter 10: Monte Carlo simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">Chapter 11: Bootstrap</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 12: Bayesian analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap13.html">Chapter 13: Markov Chains</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap14.html">Chapter 14: Substitution models</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap15.html">Chapter 15: Phylogenetic models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lab</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lab1.html">Lab 1: Introduction to R</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab2.html">Lab 2: Generating random numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab3.html">Lab 3: The law of large numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab4.html">Lab 4: BLAST</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab5.html">Lab 5: Optimization algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab6.html">Lab 6: Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab7.html">Lab 7: Multiple tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab8.html">Lab 8: RNA-seq analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab9.html">Lab 9: Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab10.html">Lab 10: Monte Carlo simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab11.html">Lab 11: Bootstrap</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab12.html">Lab 12: Markov Chain Monte Carlo algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab13.html">Lab 13: Markov chains</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab14.html">Lab 14: Substitution models</a></li>
<li class="toctree-l1"><a class="reference internal" href="lab15.html">Lab 15: Phylogenetic tree reconstruction</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchap12.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chap12.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 12: Bayesian analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">Prior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conjugate-prior">Conjugate prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-informative-prior">Non-informative prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-bayes">Empirical Bayes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sensitivity-analysis">Sensitivity analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-estimation">Bayesian estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo-algorithm">Markov Chain Monte Carlo algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#burnin">Burnin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subsampling">Subsampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference">Bayesian inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-hypothesis-testing-and-model-selection">Bayesian hypothesis testing and model selection</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-12-bayesian-analysis">
<h1>Chapter 12: Bayesian analysis<a class="headerlink" href="#chapter-12-bayesian-analysis" title="Permalink to this heading">#</a></h1>
<blockquote class="epigraph">
<div><p><em>“Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.”</em></p>
<p class="attribution">—Lao Tzu</p>
</div></blockquote>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a></p></li>
</ul>
</div>
<p>In the Bayesian framework, probability is considered as an expression of opinion, and the process of drawing inferences from data is essentially the revision of this opinion in response to pertinent new information.</p>
<div class="proof example admonition" id="exp12.1">
<p class="admonition-title"><span class="caption-number">Example 71 </span></p>
<section class="example-content" id="proof-content">
<p>There are two boxes in the room: Box 1 contains 5 red balls and 5 blue balls, while Box 2 has 1 red ball and 9 blue balls. A box is randomly selected, and 4 balls are drawn with replacement. If all selected balls are blue, the goal is to determine which box the balls were drawn from.</p>
<p><img alt="" src="_images/boxes.png" /></p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be the number of blue balls selected from the box. Given box1, <span class="math notranslate nohighlight">\(X\)</span>
follows Binomial(n=4, p=0.5). Given box2, <span class="math notranslate nohighlight">\(X\)</span> follows Binomial(n=4,
p=0.9). We assume that two boxes are equally likely to be selected,
i.e., <span class="math notranslate nohighlight">\(P(box1) = P(box2) = 0.5\)</span>.</p>
<p>Given the observed data <span class="math notranslate nohighlight">\(X=4\)</span>, we want to calculate the probability <span class="math notranslate nohighlight">\(P(box1 | X=4)\)</span>
and the probability <span class="math notranslate nohighlight">\(P(box2| X=4)\)</span>. By the Bayes rule,</p>
<div class="math notranslate nohighlight">
\[P(box1|X = 4) = \frac{P(X = 4|box1)P(box1)}{P(X = 4)}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[P(box2|X = 4) = \frac{P(X = 4|box2)P(box2)}{P(X = 4)}\]</div>
<p>We know that</p>
<div class="math notranslate nohighlight">
\[P(X = 4) = P\left( X = 4 \middle| box1 \right)P(box1) + P\left( X = 4 \middle| box2 \right)P(box2) = 0.3593\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[P(box1|X = 4) = \frac{0.03125}{0.3593} = 0.087\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[P(box2|X = 4) = \frac{0.32805}{0.3593} = 0.913\]</div>
<p>We conclude that the 4 blue balls are more likely to be selected from
box2.</p>
</section>
</div><p>In this example, the two boxes are the parameter <span class="math notranslate nohighlight">\(\theta = 1\ or\ 2\)</span>. The probability distribution of data, <span class="math notranslate nohighlight">\(P(X|\theta)\)</span>, is
called the likelihood function. The probability distribution <span class="math notranslate nohighlight">\(P(\theta = 1) = P(\theta = 2) = 0.5\)</span> is the prior distribution of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>. The Bayesian inference of
<span class="math notranslate nohighlight">\(\theta\)</span> is based on the posterior distribution of parameter <span class="math notranslate nohighlight">\(\theta\)</span>
given data <span class="math notranslate nohighlight">\(X\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[P\left( \theta \middle| X \right) = \frac{P(X|\theta)P(\theta)}{P(X)}\]</div>
<p>For the continuous data <span class="math notranslate nohighlight">\(X\)</span>, the posterior density of <span class="math notranslate nohighlight">\(\theta\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[f\left( \theta \middle| X \right) = \frac{f(X|\theta)f(\theta)}{f(X)}\]</div>
<p>The normalizing constant
<span class="math notranslate nohighlight">\(f(X) = \int_{- \infty}^{\infty}{f\left( X \middle| \theta \right)f(\theta)d\theta}\)</span>
is often intractable. Numerical approaches
(MCMC algorithms) are used to approximate the posterior distribution
<span class="math notranslate nohighlight">\(f\left( \theta \middle| X \right)\)</span> of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<section id="prior-distribution">
<h2>Prior distribution<a class="headerlink" href="#prior-distribution" title="Permalink to this heading">#</a></h2>
<section id="conjugate-prior">
<h3>Conjugate prior<a class="headerlink" href="#conjugate-prior" title="Permalink to this heading">#</a></h3>
<p>A conjugate prior is an algebraic convenience, giving a close-form expression for the posterior;
otherwise numerical approaches may be necessary.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Likelihood</p></th>
<th class="head"><p>Conjugate prior</p></th>
<th class="head"><p>Posterior</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal</p></td>
<td><p>Normal</p></td>
<td><p>Normal</p></td>
</tr>
<tr class="row-odd"><td><p>Uniform</p></td>
<td><p>Pareto</p></td>
<td><p>Pareto</p></td>
</tr>
<tr class="row-even"><td><p>Weibull</p></td>
<td><p>Inverse gamma</p></td>
<td><p>Inverse gamma</p></td>
</tr>
<tr class="row-odd"><td><p>Log-normal</p></td>
<td><p>Normal</p></td>
<td><p>Normal</p></td>
</tr>
<tr class="row-even"><td><p>Exponential</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-odd"><td><p>Inverse gamma</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-even"><td><p>Gamma</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-odd"><td><p>Binomial</p></td>
<td><p>Beta</p></td>
<td><p>Beta</p></td>
</tr>
<tr class="row-even"><td><p>Negative binomial</p></td>
<td><p>Beta</p></td>
<td><p>Beta</p></td>
</tr>
<tr class="row-odd"><td><p>Poisson</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-even"><td><p>Multinomial</p></td>
<td><p>Dirichlet</p></td>
<td><p>Dirichlet</p></td>
</tr>
</tbody>
</table>
</section>
<section id="non-informative-prior">
<h3>Non-informative prior<a class="headerlink" href="#non-informative-prior" title="Permalink to this heading">#</a></h3>
<p>For example, the uniform prior of parameter <span class="math notranslate nohighlight">\(\theta\)</span> is a non-informative prior, because all possible values of <span class="math notranslate nohighlight">\(\theta\)</span> are equally likely, no preference.</p>
</section>
<section id="empirical-bayes">
<h3>Empirical Bayes<a class="headerlink" href="#empirical-bayes" title="Permalink to this heading">#</a></h3>
<p>Empirical Bayes is a statistical methodology that integrates Bayesian principles with frequentist methods. In Empirical Bayes, parameters are estimated by maximizing the marginal likelihood, treating these estimates as if they were observed data when estimating hyperparameters. This approach effectively leverages information across various levels of the hierarchy, offering a middle ground between fully Bayesian and frequentist approaches.</p>
</section>
<section id="sensitivity-analysis">
<h3>Sensitivity analysis<a class="headerlink" href="#sensitivity-analysis" title="Permalink to this heading">#</a></h3>
<p>Bayesian inference involves considering various prior distributions to assess whether the inference is substantially influenced by different priors. If the inference remains stable across different priors, it suggests that Bayesian inference is robust to the choice of prior distribution.</p>
</section>
</section>
<section id="bayesian-estimation">
<h2>Bayesian estimation<a class="headerlink" href="#bayesian-estimation" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> be a Bayesian estimator of <span class="math notranslate nohighlight">\(\theta\)</span> and let
<span class="math notranslate nohighlight">\(L(\theta,\widehat{\theta})\)</span> be a loss function, such as the squared loss</p>
<div class="math notranslate nohighlight">
\[L\left( \theta,\widehat{\theta} \right) = \left( \theta - \widehat{\theta}(x) \right)^{2}\]</div>
<div class="proof definition admonition" id="def12.1">
<p class="admonition-title"><span class="caption-number">Definition 26 </span> (Bayes risk)</p>
<section class="definition-content" id="proof-content">
<p>The Bayes risk of <span class="math notranslate nohighlight">\(\widehat{\theta}\ \)</span>is defined as</p>
<div class="math notranslate nohighlight">
\[E\left( L\left( \theta,\widehat{\theta} \right) \right) = \int_{\theta}^{}{\int_{x}^{}{L\left( \theta,\widehat{\theta}(x) \right)f\left( x \middle| \theta \right)f(\theta)dxd\theta}}\]</div>
</section>
</div><p>The expectation is taken over the probability distribution of data <span class="math notranslate nohighlight">\(X\)</span> and parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="proof definition admonition" id="def12.2">
<p class="admonition-title"><span class="caption-number">Definition 27 </span> (Bayes estimator)</p>
<section class="definition-content" id="proof-content">
<p>An estimator <span class="math notranslate nohighlight">\(\widehat{\theta}\ \)</span>is said to be a Bayes estimator if it minimizes the Bayes risk among all estimators.</p>
</section>
</div><p>The estimator which minimizes the posterior expected loss
<span class="math notranslate nohighlight">\(E\left( L\left( \theta,\widehat{\theta} \right)|X \right)\)</span> for each <span class="math notranslate nohighlight">\(X\)</span>
also minimizes the Bayes risk and therefore is a Bayes estimator.</p>
<div class="proof theorem admonition" id="thm12.1">
<p class="admonition-title"><span class="caption-number">Theorem 20 </span></p>
<section class="theorem-content" id="proof-content">
<p>Suppose the loss function is squared error. Show that the Bayes estimator <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> of <span class="math notranslate nohighlight">\(\theta\)</span> is the posterior mean <span class="math notranslate nohighlight">\(E(\theta|X)\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The risk function is given by</p>
<div class="math notranslate nohighlight">
\[E\left( L\left( \theta,\widehat{\theta} \right)|X \right) = \int_{\theta}^{}{\left( \theta - \widehat{\theta}(x) \right)^{2}f\left( \theta \middle| x \right)d\theta}\]</div>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[\frac{\partial E\left( L\left( \theta,\widehat{\theta} \right)|X \right)}{\partial\widehat{\theta}(x)} = \int_{\theta}^{}{2\left( \theta - \widehat{\theta}(x) \right)f\left( \theta \middle| x \right)d\theta} = 2E\left( \theta \middle| x \right) - 2\widehat{\theta}(x) = 0\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[{\widehat{\theta}}_{Bayes}(x) = E(\theta|x)\]</div>
</div>
<div class="proof example admonition" id="exp12.2">
<p class="admonition-title"><span class="caption-number">Example 72 </span></p>
<section class="example-content" id="proof-content">
<p><span class="math notranslate nohighlight">\((x_{1},\ldots,x_{n})\)</span> is a random sample generated from the
exponential distribution with mean <span class="math notranslate nohighlight">\(1/\lambda\)</span>. The prior of <span class="math notranslate nohighlight">\(\lambda\)</span> is the exponential distribution with mean <span class="math notranslate nohighlight">\(1/2\)</span>. The posterior distribution of <span class="math notranslate nohighlight">\(\lambda\)</span> given <span class="math notranslate nohighlight">\((x_{1},\ldots,x_{n})\)</span> is</p>
<div class="math notranslate nohighlight">
\[f\left( \lambda \middle| X \right) = \frac{f(X|\lambda)f(\lambda)}{f(X)} = \frac{\lambda^{n}e^{- \lambda\sum_{i = 1}^{n}x_{i}}*2e^{- 2\lambda}}{f(X)} = \frac{2\lambda^{n}e^{- \left( \sum_{i = 1}^{n}x_{i} + 2 \right)\lambda}}{f(X)}\]</div>
<p>This is a gamma distribution with <span class="math notranslate nohighlight">\(\alpha = n + 1\)</span> and
<span class="math notranslate nohighlight">\(\beta = \sum_{i = 1}^{n}x_{i} + 2\)</span>. The posterior mean is
<span class="math notranslate nohighlight">\(\frac{\alpha}{\beta} = \frac{n + 1}{\sum_{i = 1}^{n}x_{i} + 2}\)</span>. Thus, the Bayes estimator of <span class="math notranslate nohighlight">\(\lambda\)</span> is <span class="math notranslate nohighlight">\(\frac{n + 1}{\sum_{i = 1}^{n}x_{i} + 2}\)</span>.</p>
<p>Suppose the data is (1.001, 0.065, 0.014, 1.601, 0.288, 0.095, 0.401, 0.227, 0.234, 0.488). Then, the Bayes estimator of <span class="math notranslate nohighlight">\(\lambda\)</span> is</p>
<div class="math notranslate nohighlight">
\[\frac{n + 1}{\sum_{i = 1}^{n}x_{i} + 2} = \frac{10 + 1}{4.41 + 2} = 1.716\]</div>
</section>
</div></section>
<section id="markov-chain-monte-carlo-algorithm">
<h2>Markov Chain Monte Carlo algorithm<a class="headerlink" href="#markov-chain-monte-carlo-algorithm" title="Permalink to this heading">#</a></h2>
<p>Markov chain Monte Carlo (MCMC) methods are a category of algorithms designed for sampling from a target probability distribution. It can be demonstrated that the samples produced by MCMC algorithms converge to the desired target probability distribution.</p>
<p>In Bayesian analysis, the focus is on the posterior distribution as the target probability distribution. Assuming the availability of the likelihood function <span class="math notranslate nohighlight">\(f(x|\theta)\)</span> and the prior <span class="math notranslate nohighlight">\(f(\theta)\)</span>, the posterior density of <span class="math notranslate nohighlight">\(\theta\)</span> is expressed as:</p>
<div class="math notranslate nohighlight">
\[f\left( \theta \middle| x \right) = \frac{f(x|\theta)f(\theta)}{f(x)}\]</div>
<p>As the normalizing constant <span class="math notranslate nohighlight">\(f(x)\)</span> is frequently challenging to compute, the posterior distribution is typically known only up to this normalizing constant. Since MCMC algorithms do not necessitate knowledge of the normalizing constant, they can approximate the posterior probability through sampling from the posterior distribution.</p>
<p>Two commonly employed MCMC algorithms are Gibbs sampling and the Metropolis-Hastings algorithm. In this context, we will outline the Metropolis-Hastings algorithm.</p>
<div class="proof algorithm admonition" id="alg12.1">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Metropolis-Hastings)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> the likelihood and prior</p>
<p><strong>Output</strong> a sample generated from the target distribution</p>
<ol class="arabic simple">
<li><p>An arbitrary initial value for <span class="math notranslate nohighlight">\(\theta = \theta_{0}\)</span></p></li>
<li><p>Update <span class="math notranslate nohighlight">\(\theta\)</span> as follows. Suppose the current value is <span class="math notranslate nohighlight">\(\theta_{n}\)</span>. A new value of <span class="math notranslate nohighlight">\(\theta\)</span> is proposed in the neighborhood of <span class="math notranslate nohighlight">\(\theta_{n}\)</span>. For example, <span class="math notranslate nohighlight">\(\theta_{new}\)</span> is
generated from the uniform distribution on <span class="math notranslate nohighlight">\(\lbrack\theta_{n} - c,\theta_{n} + c\rbrack\)</span>. The proposal distribution is the probability distribution from which <span class="math notranslate nohighlight">\(\theta_{new}\)</span> is proposed, written as <span class="math notranslate nohighlight">\(P(\theta_{new}|\theta_{n})\)</span>. Here, we use the uniform distribution (it is often called random walk) as the proposal distribution.</p></li>
<li><p>The newly proposed <span class="math notranslate nohighlight">\(\theta_{new}\)</span> is either accepted or rejected according to the probability known as the Hastings ratio,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\theta_{n + 1} = \begin{matrix}
\begin{cases}
\theta_{new}\ with\ probabilty = \ min\left\{\frac{f\left( \theta_{new} \middle| x \right)P\left( \theta_{n} \middle| \theta_{new} \right)}{f\left( \theta_{n} \middle| x \right)P\left( \theta_{new} \middle| \theta_{n} \right)},\ 1\right\} \\
\theta_{n},\ otherwise \\
\end{cases}
\end{matrix}
\end{split}\]</div>
<ol class="arabic simple" start="4">
<li><p>Continue to generate <span class="math notranslate nohighlight">\(\theta\)</span> until the algorithm converges</p></li>
</ol>
</section>
</div><section id="burnin">
<h3>Burnin<a class="headerlink" href="#burnin" title="Permalink to this heading">#</a></h3>
<p>Various methods have been devised to assess the convergence of MCMC algorithms. A straightforward approach involves creating a log-likelihood plot. The log-likelihood typically shows a continuous increase until it stabilizes, signaling the convergence of the MCMC algorithm. The period before the chain reaches convergence is termed the “burn-in” phase. Samples generated during the burn-in phase are typically discarded.</p>
</section>
<section id="subsampling">
<h3>Subsampling<a class="headerlink" href="#subsampling" title="Permalink to this heading">#</a></h3>
<p>It is important to recognize that samples produced by MCMC algorithms are not independent; they exhibit dependence as the new value <span class="math notranslate nohighlight">\(\theta_{new}\)</span> is proposed from the vicinity of the old value <span class="math notranslate nohighlight">\(\theta_{n}\)</span>. To alleviate this dependency, subsampling of <span class="math notranslate nohighlight">\(\theta\)</span> is often employed, such as sampling every 1000th <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</section>
<section id="bayesian-inference">
<h3>Bayesian inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this heading">#</a></h3>
<p>After obtaining a sample of <span class="math notranslate nohighlight">\(\theta\)</span> from the posterior distribution, Bayesian inference can be conducted based on this generated sample. For instance, the posterior mean, which serves as a Bayesian estimator for parameter <span class="math notranslate nohighlight">\(\theta\)</span>, can be approximated by the sample average of <span class="math notranslate nohighlight">\(\theta\)</span> derived from the MCMC algorithm.</p>
</section>
</section>
<section id="bayesian-hypothesis-testing-and-model-selection">
<h2>Bayesian hypothesis testing and model selection<a class="headerlink" href="#bayesian-hypothesis-testing-and-model-selection" title="Permalink to this heading">#</a></h2>
<p>We assume that data <span class="math notranslate nohighlight">\(X\)</span> have arisen from one of the two hypotheses
<span class="math notranslate nohighlight">\(H_0\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> according to a probability density <span class="math notranslate nohighlight">\(P(X|H_{0})\)</span> or
<span class="math notranslate nohighlight">\(P(X|H_{1})\)</span>. Given a priori probabilities <span class="math notranslate nohighlight">\(P(H_{0})\)</span> and
<span class="math notranslate nohighlight">\(P\left( H_{1} \right) = 1 - P(H_{0})\)</span>, the posterior probability of hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[P\left( H_{0} \middle| X \right) = \frac{P(X|H_{0})P(H_{0})}{P(X)}\]</div>
<p>Thus, the posterior odds is equal to</p>
<div class="math notranslate nohighlight">
\[\frac{P\left( H_{0} \middle| X \right)}{P(H_{1}|X)} = \frac{P(X|H_{0})P(H_{0})}{P(X|H_{1})P(H_{1})}\]</div>
<p>The Bayes factor is <span class="math notranslate nohighlight">\(BF_{01} = \frac{P(X|H_{0})}{P(X|H_{1})}\)</span>.
When <span class="math notranslate nohighlight">\(H_0\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> are equally probable, the posterior odds is equal to the Bayes factor. In addition, <span class="math notranslate nohighlight">\(P\left( X \middle| H_{0} \right)\)</span> is the marginal likelihood under <span class="math notranslate nohighlight">\(H_0\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[P\left( X \middle| H_{0} \right) = \int_{- \infty}^{\infty}{P\left( X \middle| \theta_{0},H_{0} \right)P\left( \theta_{0} \middle| H_{0} \right)d\theta_{0}}\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(P\left( X \middle| H_{1} \right)\)</span> is the marginal
likelihood under <span class="math notranslate nohighlight">\(H_1\)</span>. Thus, the Bayes factor is the marginal likelihood ratio statistic.</p>
<p>The Bayes factor serves as a summary of the evidence offered by the data in support of one scientific theory over another. The interpretation of the Bayes factor is crucial in assessing the relative strength of evidence for competing hypotheses.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(log_{10}(B_{10})\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(B_{10}\)</span></p></th>
<th class="head"><p>Evidence against <span class="math notranslate nohighlight">\(H_0\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0 to 1/2</p></td>
<td><p>1 to 3.2</p></td>
<td><p>Not worth more than a bare mention</p></td>
</tr>
<tr class="row-odd"><td><p>½ to 1</p></td>
<td><p>3.2 to 10</p></td>
<td><p>substantial</p></td>
</tr>
<tr class="row-even"><td><p>1 to 2</p></td>
<td><p>10 to 100</p></td>
<td><p>Strong</p></td>
</tr>
<tr class="row-odd"><td><p>&gt;2</p></td>
<td><p>&gt;100</p></td>
<td><p>decisive</p></td>
</tr>
</tbody>
</table>
<p>The marginal likelihoods involved in the Bayes factor are frequently challenging to compute directly. Numerical methods are employed to approximate the Bayes factor. One straightforward approximation technique is the harmonic mean approximation,</p>
<div class="math notranslate nohighlight">
\[\frac{1}{P\left( X \middle| H_{0} \right)} = \int_{- \infty}^{\infty}{\frac{1}{P\left( X \middle| \theta_{0},H_{0} \right)}P\left( \theta_{0}|H_{0} \right)d\theta_{0}} = E\left( \frac{1}{P\left( X \middle| \theta_{0},H_{0} \right)} \right)\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\widehat{P\left( X \middle| H_{0} \right)} = \left\lbrack \frac{1}{n}\sum_{i = 1}^{m}\left( \frac{1}{P\left( X \middle| \theta_{0}^{i},H_{0} \right)} \right) \right\rbrack^{- 1}\]</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="chap11.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 11: Bootstrap</p>
      </div>
    </a>
    <a class="right-next"
       href="chap13.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 13: Markov Chains</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">Prior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conjugate-prior">Conjugate prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-informative-prior">Non-informative prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-bayes">Empirical Bayes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sensitivity-analysis">Sensitivity analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-estimation">Bayesian estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo-algorithm">Markov Chain Monte Carlo algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#burnin">Burnin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subsampling">Subsampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference">Bayesian inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-hypothesis-testing-and-model-selection">Bayesian hypothesis testing and model selection</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Liang Liu
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>