
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Chapter 12: Bayesian analysis &#8212; Statistical Inference in Bioinformatics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 13: Markov Chains" href="chap13.html" />
    <link rel="prev" title="Chapter 11: Bootstrap" href="chap11.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistical Inference in Bioinformatics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Table of Contents
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chap0.html">
   Chapter 0: Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap1.html">
   Chapter 1: Probability theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap2.html">
   Chapter 2: Discrete random variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap3.html">
   Chapter 3: Continuous random variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap4.html">
   Chapter 4: Multiple random variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap5.html">
   Chapter 5: Estimation theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap6.html">
   Chapter 6: Hypothesis testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap7.html">
   Chapter 7: Multiple tests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap8.html">
   Chapter 8: RNA-seq analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap9.html">
   Chapter 9: Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap10.html">
   Chapter 10: Monte Carlo simulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap11.html">
   Chapter 11: Bootstrap
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 12: Bayesian analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap13.html">
   Chapter 13: Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap14.html">
   Chapter 14: Substitution models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap15.html">
   Chapter 15: Phylogenetic models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lab
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="lab1.html">
   Lab 1: Introduction to R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab2.html">
   Lab 2: Generating random numbers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab3.html">
   Lab 3: The law of large numbers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab4.html">
   Lab 4: GenBank
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab5.html">
   Lab 5: Maixmum Likelihood Estimates
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab6.html">
   Lab 6: Hypothesis testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab7.html">
   Lab 7: Multiple tests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab8.html">
   Lab 8: RNA-seq analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab9.html">
   Lab 9: Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab10.html">
   Lab 10: Monte Carlo simulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab11.html">
   Lab 11: Bootstrap
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab12.html">
   Lab 12: Markov Chain Monte Carlo algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab13.html">
   Lab 13: Markov chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab14.html">
   Lab 14: Substitution models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lab15.html">
   Lab 15: Phylogenetic tree reconstruction
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/chap12.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchap12.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/chap12.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="_sources/chap12.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-distribution">
   Prior distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conjugate-prior">
     1. conjugate prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-informative-prior">
     2. non-informative prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#empirical-bayesian">
     3. Empirical Bayesian
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sensitivity-analysis">
     4. Sensitivity analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-estimation">
   Bayesian estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-chain-monte-carlo-algorithm">
   Markov Chain Monte Carlo algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#burnin">
     1. Burnin
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subsampling">
     2. Subsampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-inference">
     3. Bayesian inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-hypothesis-testing-and-model-selection">
   Bayesian hypothesis testing and model selection
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Chapter 12: Bayesian analysis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-distribution">
   Prior distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conjugate-prior">
     1. conjugate prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-informative-prior">
     2. non-informative prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#empirical-bayesian">
     3. Empirical Bayesian
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sensitivity-analysis">
     4. Sensitivity analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-estimation">
   Bayesian estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-chain-monte-carlo-algorithm">
   Markov Chain Monte Carlo algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#burnin">
     1. Burnin
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#subsampling">
     2. Subsampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-inference">
     3. Bayesian inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-hypothesis-testing-and-model-selection">
   Bayesian hypothesis testing and model selection
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-12-bayesian-analysis">
<h1>Chapter 12: Bayesian analysis<a class="headerlink" href="#chapter-12-bayesian-analysis" title="Permalink to this headline">#</a></h1>
<blockquote class="epigraph">
<div><p><em>“Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step.”</em></p>
<p class="attribution">—Lao Tzu</p>
</div></blockquote>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a></p></li>
</ul>
</div>
<p>In the Bayesian framework, probability is
opinion, and that inference from data is nothing other than the revision
of such opinion in the light of relevant new information.</p>
<div class="proof example admonition" id="12.1">
<p class="admonition-title"><span>Example </span> (12.1)</p>
<section class="example-content" id="proof-content">
<p>There are two boxes in the room: 5 red balls and 5 blue balls
in box1; 1 red ball and 9 blue balls in box2. Someone randomly selected
a box and took 4 balls with replacement from the box. Suppose all
selected balls are blue. We want to know the box from which the balls
were selected.</p>
<p><img alt="" src="_images/boxes.png" /></p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be the number of blue balls selected from the box. Given box1, X
follows Binomial(n=4, p=0.5). Given box2, <span class="math notranslate nohighlight">\(X\)</span> follows Binomial(n=4,
p=0.9). We assume that two boxes are equally likely to be selected,
i.e., <span class="math notranslate nohighlight">\(P(box1) = P(box2) = 0.5\)</span>.</p>
<p>Given the observed data <span class="math notranslate nohighlight">\(X=4\)</span>, we want to calculate the probability <span class="math notranslate nohighlight">\(P(box1 | X=4)\)</span>
and the probability <span class="math notranslate nohighlight">\(P(box2| X=4)\)</span>. By the Bayes rule,</p>
<div class="math notranslate nohighlight">
\[P(box1|X = 4) = \frac{P(X = 4|box1)P(box1)}{P(X = 4)}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[P(box2|X = 4) = \frac{P(X = 4|box2)P(box2)}{P(X = 4)}\]</div>
<p>We know that</p>
<div class="math notranslate nohighlight">
\[P(X = 4) = P\left( X = 4 \middle| box1 \right)P(box1) + P\left( X = 4 \middle| box2 \right)P(box2) = 0.3593\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[P(box1|X = 4) = \frac{0.03125}{0.3593} = 0.087\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[P(box2|X = 4) = \frac{0.32805}{0.3593} = 0.913\]</div>
<p>We conclude that the 4 blue balls are more likely to be selected from
box2.</p>
</section>
</div><p>In this example, the two boxes are the parameter <span class="math notranslate nohighlight">\(\theta = 1\ or\ 2\)</span>. The probability distribution of data, <span class="math notranslate nohighlight">\(P(X|\theta)\)</span>, is
called the likelihood function. The probability distribution <span class="math notranslate nohighlight">\(P(\theta = 1) = P(\theta = 2) = 0.5\)</span> is the prior distribution of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>. The Bayesian inference of
<span class="math notranslate nohighlight">\(\theta\)</span> is based on the posterior distribution of parameter <span class="math notranslate nohighlight">\(\theta\)</span>
given data X, i.e.,</p>
<div class="math notranslate nohighlight">
\[P\left( \theta \middle| X \right) = \frac{P(X|\theta)P(\theta)}{P(X)}\]</div>
<p>For the continuous data <span class="math notranslate nohighlight">\(X\)</span>, the posterior density of <span class="math notranslate nohighlight">\(\theta\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[f\left( \theta \middle| X \right) = \frac{f(X|\theta)f(\theta)}{f(X)}\]</div>
<p>The normalizing constant
<span class="math notranslate nohighlight">\(f(X) = \int_{- \infty}^{\infty}{f\left( X \middle| \theta \right)f(\theta)d\theta}\)</span>
is often intractable. Numerical approaches
(MCMC algorithms) are used to approximate the posterior distribution
<span class="math notranslate nohighlight">\(f\left( \theta \middle| X \right)\)</span> of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<section id="prior-distribution">
<h2>Prior distribution<a class="headerlink" href="#prior-distribution" title="Permalink to this headline">#</a></h2>
<section id="conjugate-prior">
<h3>1. conjugate prior<a class="headerlink" href="#conjugate-prior" title="Permalink to this headline">#</a></h3>
<p>A conjugate prior is an algebraic convenience, giving a close-form expression for the posterior;
otherwise numerical approaches may be necessary.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Likelihood</p></th>
<th class="head"><p>Conjugate prior</p></th>
<th class="head"><p>Posterior</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal</p></td>
<td><p>Normal</p></td>
<td><p>Normal</p></td>
</tr>
<tr class="row-odd"><td><p>Uniform</p></td>
<td><p>Pareto</p></td>
<td><p>Pareto</p></td>
</tr>
<tr class="row-even"><td><p>Weibull</p></td>
<td><p>Inverse gamma</p></td>
<td><p>Inverse gamma</p></td>
</tr>
<tr class="row-odd"><td><p>Log-normal</p></td>
<td><p>Normal</p></td>
<td><p>Normal</p></td>
</tr>
<tr class="row-even"><td><p>Exponential</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-odd"><td><p>Inverse gamma</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-even"><td><p>Gamma</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma\</p></td>
</tr>
<tr class="row-odd"><td><p>Binomial</p></td>
<td><p>Beta</p></td>
<td><p>Beta</p></td>
</tr>
<tr class="row-even"><td><p>Negative binomial</p></td>
<td><p>Beta</p></td>
<td><p>Beta</p></td>
</tr>
<tr class="row-odd"><td><p>Poisson</p></td>
<td><p>Gamma</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-even"><td><p>Multinomial</p></td>
<td><p>Dirichlet</p></td>
<td><p>Dirichlet</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</section>
<section id="non-informative-prior">
<h3>2. non-informative prior<a class="headerlink" href="#non-informative-prior" title="Permalink to this headline">#</a></h3>
<p>For example, the uniform prior of parameter <span class="math notranslate nohighlight">\(\theta\)</span> is a non-informative prior, because all possible values of <span class="math notranslate nohighlight">\(\theta\)</span> are equally likely, no preference.</p>
</section>
<section id="empirical-bayesian">
<h3>3. Empirical Bayesian<a class="headerlink" href="#empirical-bayesian" title="Permalink to this headline">#</a></h3>
<p>The prior distribution <span class="math notranslate nohighlight">\(P(\theta)\)</span> is estimated from data</p>
</section>
<section id="sensitivity-analysis">
<h3>4. Sensitivity analysis<a class="headerlink" href="#sensitivity-analysis" title="Permalink to this headline">#</a></h3>
<p>The Bayesian inference is based on many prior distributions to see if the inference is significantly affected by different priors. If not, it indicates that the Bayesian inference is robust to the prior distribution.</p>
</section>
</section>
<section id="bayesian-estimation">
<h2>Bayesian estimation<a class="headerlink" href="#bayesian-estimation" title="Permalink to this headline">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> be a Bayesian estimator of <span class="math notranslate nohighlight">\(\theta\)</span> and let
<span class="math notranslate nohighlight">\(L(\theta,\widehat{\theta})\)</span> be a loss function, such as the squared loss</p>
<div class="math notranslate nohighlight">
\[L\left( \theta,\widehat{\theta} \right) = \left( \theta - \widehat{\theta}(x) \right)^{2}\]</div>
<div class="proof definition admonition" id="Bayes risk">
<p class="admonition-title"><span>Definition </span> (Bayes risk)</p>
<section class="definition-content" id="proof-content">
<p>The Bayes risk of <span class="math notranslate nohighlight">\(\widehat{\theta}\ \)</span>is defined as</p>
<div class="math notranslate nohighlight">
\[E\left( L\left( \theta,\widehat{\theta} \right) \right) = \int_{\theta}^{}{\int_{x}^{}{L\left( \theta,\widehat{\theta}(x) \right)f\left( x \middle| \theta \right)f(\theta)dxd\theta}}\]</div>
</section>
</div><p>The expectation is taken over the probability distribution of data <span class="math notranslate nohighlight">\(X\)</span> and parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="proof definition admonition" id="Bayes estimator">
<p class="admonition-title"><span>Definition </span> (Bayes estimator)</p>
<section class="definition-content" id="proof-content">
<p>An estimator <span class="math notranslate nohighlight">\(\widehat{\theta}\ \)</span>is said to be a Bayes estimator if it
minimizes the Bayes risk among all estimators.</p>
</section>
</div><p>The estimator which minimizes the posterior expected loss
<span class="math notranslate nohighlight">\(E\left( L\left( \theta,\widehat{\theta} \right)|X \right)\)</span> for each <span class="math notranslate nohighlight">\(X\)</span>
also minimizes the Bayes risk and therefore is a Bayes estimator.</p>
<div class="proof example admonition" id="12.2">
<p class="admonition-title"><span>Example </span> (12.2)</p>
<section class="example-content" id="proof-content">
<p>If the loss function is squared error, the Bayesian estimate
<span class="math notranslate nohighlight">\(\widehat{\theta}\)</span> of <span class="math notranslate nohighlight">\(\theta\)</span> is the posterior mean <span class="math notranslate nohighlight">\(E(\theta|X)\)</span>,
because the risk function is given by</p>
<div class="math notranslate nohighlight">
\[E\left( L\left( \theta,\widehat{\theta} \right)|X \right) = \int_{\theta}^{}{\left( \theta - \widehat{\theta}(x) \right)^{2}f\left( \theta \middle| x \right)d\theta}\]</div>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[\frac{\partial E\left( L\left( \theta,\widehat{\theta} \right)|X \right)}{\partial\widehat{\theta}(x)} = \int_{\theta}^{}{2\left( \theta - \widehat{\theta}(x) \right)f\left( \theta \middle| x \right)d\theta} = 2E\left( \theta \middle| x \right) - 2\widehat{\theta}(x) = 0\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[{\widehat{\theta}}_{Bayes}(x) = E(\theta|x)\]</div>
</section>
</div></section>
<section id="markov-chain-monte-carlo-algorithm">
<h2>Markov Chain Monte Carlo algorithm<a class="headerlink" href="#markov-chain-monte-carlo-algorithm" title="Permalink to this headline">#</a></h2>
<p>Markov chain Monte Carlo (MCMC) methods are a class of algorithms for
sampling from a target probability distribution. It can be shown that
the samples generated from the MCMC algorithms converge to the target
probability distribution.</p>
<p>In Bayesian analysis, the target probability distribution is the
posterior distribution. Suppose the likelihood function <span class="math notranslate nohighlight">\(f(x|\theta)\)</span>
and prior <span class="math notranslate nohighlight">\(f(\theta)\)</span> are given. The posterior density of <span class="math notranslate nohighlight">\(\theta\)</span> is
given by</p>
<div class="math notranslate nohighlight">
\[f\left( \theta \middle| x \right) = \frac{f(x|\theta)f(\theta)}{f(x)}\]</div>
<p>Since the normalizing constant <span class="math notranslate nohighlight">\(f(x)\)</span> is often intractable, the
posterior distribution is known up to the normalizing constant. Because
the MCMC algorithms do not require to know the normalizing constant, they can approximate the posterior probability by sampling from the posterior distribution.</p>
<p>The commonly used MCMC algorithms include Gibbs sampling and
Metropolis-Hastings algorithm. We here describe the
Metropolis-Hastings algorithm</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span>Algorithm </span> (Metropolis-Hastings)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> the likelihood and prior
<strong>Output</strong> a sample generated from the target distribution</p>
<ol>
<li><p>An arbitrary initial value for <span class="math notranslate nohighlight">\(\theta = \theta_{0}\)</span></p></li>
<li><p>We update <span class="math notranslate nohighlight">\(\theta\)</span> as follows. Suppose the current value is
<span class="math notranslate nohighlight">\(\theta_{n}\)</span>. A new value of <span class="math notranslate nohighlight">\(\theta\)</span> is proposed in the
neighborhood of <span class="math notranslate nohighlight">\(\theta_{n}\)</span>. For example, <span class="math notranslate nohighlight">\(\theta_{new}\)</span> is
generated from the uniform distribution on
<span class="math notranslate nohighlight">\(\lbrack\theta_{n} - c,\theta_{n} + c\rbrack\)</span>. The proposal
distribution is the probability distribution from which
<span class="math notranslate nohighlight">\(\theta_{new}\)</span> is proposed, written as <span class="math notranslate nohighlight">\(P(\theta_{new}|\theta_{n})\)</span>.
Here, we use the uniform distribution (it is often called random
walk) as the proposal distribution.</p></li>
<li><p>The newly proposed <span class="math notranslate nohighlight">\(\theta_{new}\)</span> is either accepted or rejected according to the probability known as the Hastings ratio,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \theta_{n + 1} = \begin{matrix}
    \begin{cases}
    \theta_{new}\ with\ probabilty = \ min\left\{\frac{f\left( \theta_{new} \middle| x \right)P\left( \theta_{n} \middle| \theta_{new} \right)}{f\left( \theta_{n} \middle| x \right)P\left( \theta_{new} \middle| \theta_{n} \right)},\ 1\right\} \\
    \theta_{n},\ otherwise \\
    \end{cases}
    \end{matrix}
    \end{split}\]</div>
</li>
<li><p>Continue to generate <span class="math notranslate nohighlight">\(\theta\)</span> until the algorithm converges</p></li>
</ol>
</section>
</div><section id="burnin">
<h3>1. Burnin<a class="headerlink" href="#burnin" title="Permalink to this headline">#</a></h3>
<p>Many approaches have been developed for checking the convergence of the MCMC algorithms.
A simple method is to make a log-likelihood plot. The log-likelihood
continues to increase and then it will become stable at some point,
indicating that the MCMC algorithm has converged. The time period before
the chain gets converged is called “burn-in”. The samples generated
during burn-in should be discarded.</p>
</section>
<section id="subsampling">
<h3>2. Subsampling<a class="headerlink" href="#subsampling" title="Permalink to this headline">#</a></h3>
<p>Note that the samples generated from the MCMC algorithms are not random
samples. They are dependent of each other, because the new value
<span class="math notranslate nohighlight">\(\theta_{new}\)</span> is proposed from the neighborhood of the old value <span class="math notranslate nohighlight">\(\theta_{n}\)</span>. To
reduce dependency, we subsample <span class="math notranslate nohighlight">\(\theta\)</span>, for example, we sample every 1000 <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</section>
<section id="bayesian-inference">
<h3>3. Bayesian inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">#</a></h3>
<p>Once we have a sample of <span class="math notranslate nohighlight">\(\theta\)</span> generated from the posterior
distribution, the Bayesian inference can be based on the generated
sample. For example, the posterior mean, a Bayesian estimator of
parameter <span class="math notranslate nohighlight">\(\theta\)</span>, is approximated by the sample average of <span class="math notranslate nohighlight">\(\theta\)</span>
generated from the MCMC algorithm.</p>
<div class="proof example admonition" id="12.3">
<p class="admonition-title"><span>Example </span> (12.3)</p>
<section class="example-content" id="proof-content">
<p><span class="math notranslate nohighlight">\((x_{1},\ldots,x_{n})\)</span> is a random sample generated from the
exponential distribution with mean <span class="math notranslate nohighlight">\(1/\lambda\)</span>. The prior of <span class="math notranslate nohighlight">\(\lambda\)</span>
is the exponential distribution with mean 1/2. The posterior
distribution of <span class="math notranslate nohighlight">\(\lambda\)</span> given <span class="math notranslate nohighlight">\((x_{1},\ldots,x_{n})\)</span> is</p>
<div class="math notranslate nohighlight">
\[f\left( \lambda \middle| X \right) = \frac{f(X|\lambda)f(\lambda)}{f(X)} = \frac{\lambda^{n}e^{- \lambda\sum_{i = 1}^{n}x_{i}}*2e^{- 2\lambda}}{f(X)} = \frac{2\lambda^{n}e^{- \left( \sum_{i = 1}^{n}x_{i} + 2 \right)\lambda}}{f(X)}\]</div>
<p>This is a gamma distribution with <span class="math notranslate nohighlight">\(\alpha = n + 1\)</span> and
<span class="math notranslate nohighlight">\(\beta = \sum_{i = 1}^{n}x_{i} + 2\)</span>. The posterior mean is
<span class="math notranslate nohighlight">\(\frac{\alpha}{\beta} = \frac{n + 1}{\sum_{i = 1}^{n}x_{i} + 2}\)</span>. Thus,
the Bayesian estimate of <span class="math notranslate nohighlight">\(\lambda\)</span> is
<span class="math notranslate nohighlight">\(\frac{n + 1}{\sum_{i = 1}^{n}x_{i} + 2}\)</span>.</p>
<p>Suppose the data is (1.001, 0.065, 0.014, 1.601, 0.288, 0.095, 0.401,
0.227, 0.234, 0.488). Then, the Bayesian estimate of <span class="math notranslate nohighlight">\(\lambda\)</span> is</p>
<div class="math notranslate nohighlight">
\[\frac{n + 1}{\sum_{i = 1}^{n}x_{i} + 2} = \frac{10 + 1}{4.41 + 2} = 1.716\]</div>
</section>
</div><p>Let’s use the MCMC algorithm to approximate the posterior distribution
<span class="math notranslate nohighlight">\(f\left( \lambda \middle| X \right)\)</span>, then calculate the posterior mean.</p>
<p>The R code of the MCMC algorithm should have the following functions (1)
Likelihood function, (2) Prior, (3) a function for updating <span class="math notranslate nohighlight">\(\lambda\)</span>,
and (4) a function for accepting or rejecting the proposed
<span class="math notranslate nohighlight">\(\lambda_{new}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">########################################################</span>
<span class="c1"># mcmc algorithm: likelihood: exp, prior: exp</span>
<span class="c1">########################################################</span>
<span class="n">loglikelihood</span> <span class="o">&lt;-</span> <span class="nf">function </span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">n</span> <span class="o">&lt;-</span> <span class="nf">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">loglike</span> <span class="o">&lt;-</span> <span class="n">n</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">lambda</span><span class="p">)</span> <span class="o">-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">lambda</span>
  <span class="nf">return </span><span class="p">(</span><span class="n">loglike</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">logprior</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">logprior</span> <span class="o">&lt;-</span> <span class="nf">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">lambda</span><span class="o">*</span><span class="n">theta</span>
  <span class="nf">return </span><span class="p">(</span><span class="n">logprior</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">update_lambda</span><span class="o">&lt;-</span><span class="nf">function</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span> <span class="n">window_width</span><span class="p">){</span>
  <span class="n">newlambda</span> <span class="o">&lt;-</span> <span class="n">lambda</span> <span class="o">+</span> <span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="nf">runif</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="m">-1</span><span class="p">)</span> <span class="o">*</span> <span class="n">window_width</span>
  <span class="nf">return </span><span class="p">(</span><span class="n">newlambda</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1">#######################################################</span>
<span class="c1"># algorithm</span>
<span class="c1">########################################################</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">rexp</span><span class="p">(</span><span class="m">100</span><span class="p">)</span>
<span class="n">samplesize</span> <span class="o">=</span> <span class="nf">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">&lt;-</span> <span class="m">10</span> <span class="c1">#prior of lambda</span>
<span class="n">lambda_theory</span> <span class="o">=</span> <span class="p">(</span><span class="n">samplesize</span><span class="m">+1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">theta</span><span class="p">)</span>

<span class="n">totalround</span> <span class="o">&lt;-</span> <span class="m">100000</span>
<span class="n">lambda</span> <span class="o">&lt;-</span> <span class="m">1</span><span class="o">:</span><span class="n">totalround</span>
<span class="n">loglike</span> <span class="o">&lt;-</span> <span class="m">1</span><span class="o">:</span><span class="n">totalround</span>
<span class="n">window_width</span> <span class="o">&lt;-</span> <span class="m">0.05</span>

<span class="n">oldlambda</span> <span class="o">&lt;-</span> <span class="m">1</span> <span class="c1">#initial value of lambda</span>
<span class="n">oldloglike</span> <span class="o">&lt;-</span> <span class="nf">loglikelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">oldlambda</span><span class="p">)</span>
<span class="n">oldlogprior</span> <span class="o">&lt;-</span> <span class="nf">logprior</span><span class="p">(</span><span class="n">oldlambda</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">totalround</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">newlambda</span> <span class="o">&lt;-</span> <span class="nf">update_lambda</span><span class="p">(</span><span class="n">oldlambda</span><span class="p">,</span> <span class="n">window_width</span><span class="p">)</span>
  <span class="nf">if </span><span class="p">(</span><span class="n">newlambda</span><span class="o">&lt;</span><span class="m">0</span><span class="p">)</span> <span class="n">newlambda</span> <span class="o">=</span> <span class="o">-</span><span class="n">newlambda</span>
  <span class="n">newloglike</span> <span class="o">&lt;-</span> <span class="nf">loglikelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">newlambda</span><span class="p">)</span>
  <span class="n">newlogprior</span> <span class="o">&lt;-</span> <span class="nf">logprior</span><span class="p">(</span><span class="n">newlambda</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

  <span class="n">hastings_ratio</span> <span class="o">&lt;-</span> <span class="nf">min</span><span class="p">(</span><span class="nf">exp</span><span class="p">((</span><span class="n">newloglike</span><span class="o">+</span><span class="n">newlogprior</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="n">oldloglike</span><span class="o">+</span><span class="n">oldlogprior</span><span class="p">)),</span><span class="m">1</span><span class="p">)</span>

  <span class="nf">if</span><span class="p">(</span><span class="nf">runif</span><span class="p">(</span><span class="m">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">hastings_ratio</span><span class="p">){</span>
    <span class="n">lambda</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">newlambda</span>
    <span class="n">loglike</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">newloglike</span>
    <span class="n">oldlambda</span> <span class="o">=</span> <span class="n">newlambda</span>
    <span class="n">oldlogprior</span> <span class="o">=</span> <span class="n">newlogprior</span>
  <span class="p">}</span><span class="n">else</span><span class="p">{</span>
    <span class="n">lambda</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">oldlambda</span>
    <span class="n">loglike</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">oldloglike</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">loglike</span><span class="p">,</span><span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">)</span>
<span class="n">burnin</span> <span class="o">=</span> <span class="n">totalround</span><span class="o">/</span><span class="m">2</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;MCMC estimate of lambda:&quot;</span><span class="p">,</span> <span class="nf">mean</span><span class="p">(</span><span class="n">lambda</span><span class="p">[</span><span class="n">burnin</span><span class="o">:</span><span class="n">totalround</span><span class="p">])))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Bayesian estimate of lambda:&quot;</span><span class="p">,</span><span class="n">lambda_theory</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;MCMC estimate of lambda: 0.827577320468705&quot;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Bayesian estimate of lambda: 0.879404040212836&quot;
</pre></div>
</div>
<img alt="_images/chap12_1_2.png" src="_images/chap12_1_2.png" />
</div>
</div>
</section>
</section>
<section id="bayesian-hypothesis-testing-and-model-selection">
<h2>Bayesian hypothesis testing and model selection<a class="headerlink" href="#bayesian-hypothesis-testing-and-model-selection" title="Permalink to this headline">#</a></h2>
<p>We assume that data <span class="math notranslate nohighlight">\(X\)</span> have arisen from one of the two hypotheses
<span class="math notranslate nohighlight">\(H_0\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> according to a probability density <span class="math notranslate nohighlight">\(P(X|H_{0})\)</span> or
<span class="math notranslate nohighlight">\(P(X|H_{1})\)</span>.</p>
<p>Given a priori probabilities <span class="math notranslate nohighlight">\(P(H_{0})\)</span> and
<span class="math notranslate nohighlight">\(P\left( H_{1} \right) = 1 - P(H_{0})\)</span>, the posterior probability of
hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[P\left( H_{0} \middle| X \right) = \frac{P(X|H_{0})P(H_{0})}{P(X)}\]</div>
<p>Thus, the posterior odds is equal to</p>
<div class="math notranslate nohighlight">
\[\frac{P\left( H_{0} \middle| X \right)}{P(H_{1}|X)} = \frac{P(X|H_{0})P(H_{0})}{P(X|H_{1})P(H_{1})}\]</div>
<p>The Bayes factor is <span class="math notranslate nohighlight">\(BF_{01} = \frac{P(X|H_{0})}{P(X|H_{1})}\)</span>.
When <span class="math notranslate nohighlight">\(H_0\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> are equally probable, the posterior odds is equal to
the Bayes factor. In addition,</p>
<div class="math notranslate nohighlight">
\[P\left( X \middle| H_{0} \right) = \int_{- \infty}^{\infty}{P\left( X \middle| \theta_{0},H_{0} \right)P\left( \theta_{0} \middle| H_{0} \right)d\theta_{0}}\]</div>
<p><span class="math notranslate nohighlight">\(P\left( X \middle| H_{0} \right)\)</span> is the marginal likelihood under
<span class="math notranslate nohighlight">\(H_0\)</span>. Similarly, <span class="math notranslate nohighlight">\(P\left( X \middle| H_{1} \right)\)</span> is the marginal
likelihood under <span class="math notranslate nohighlight">\(H_1\)</span>. Thus, the Bayes factor is the marginal likelihood
ratio statistic.</p>
<p>The Bayes factor is the summary of the evidence provided by the data in
favor of one scientific theory as opposed to another. Interpretation of
the Bayes factor</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(log_{10}(B_{10})\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(B_{10}\)</span></p></th>
<th class="head"><p>Evidence against <span class="math notranslate nohighlight">\(H_0\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0 to 1/2</p></td>
<td><p>1 to 3.2</p></td>
<td><p>Not worth more than a bare mention</p></td>
</tr>
<tr class="row-odd"><td><p>½ to 1</p></td>
<td><p>3.2 to 10</p></td>
<td><p>substantial</p></td>
</tr>
<tr class="row-even"><td><p>1 to 2</p></td>
<td><p>10 to 100</p></td>
<td><p>Strong</p></td>
</tr>
<tr class="row-odd"><td><p>&gt;2</p></td>
<td><p>&gt;100</p></td>
<td><p>decisive</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The marginal likelihoods in the Bayes factor are often intractable. We
use numerical approaches to approximate the Bayes factor. A simple
approximation method is the harmonic mean approximation,</p>
<div class="math notranslate nohighlight">
\[\frac{1}{P\left( X \middle| H_{0} \right)} = \int_{- \infty}^{\infty}{\frac{1}{P\left( X \middle| \theta_{0},H_{0} \right)}P\left( \theta_{0}|H_{0} \right)d\theta_{0}} = E\left( \frac{1}{P\left( X \middle| \theta_{0},H_{0} \right)} \right)\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\widehat{P\left( X \middle| H_{0} \right)} = \left\lbrack \frac{1}{n}\sum_{i = 1}^{m}\left( \frac{1}{P\left( X \middle| \theta_{0}^{i},H_{0} \right)} \right) \right\rbrack^{- 1}\]</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="chap11.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Chapter 11: Bootstrap</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="chap13.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 13: Markov Chains</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Liang Liu<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>